<!DOCTYPE html>
<html lang="en">

<!-- Head tag (contains Google-Analytics、Baidu-Tongji)-->
<head>
  <!-- Google Analytics -->
  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-5GGTGE8BJS"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());

      gtag('config', 'G-5GGTGE8BJS');
    </script>
  

  <!-- Baidu Tongji -->
  

  <!-- Baidu Push -->
  

  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>

  <!-- <meta name="google-site-verification" content="lxDfCplOZbIzjhG34NuQBgu2gdyRlAtMB4utP5AgEBc"/> -->
  <meta name="google-site-verification" content="MnOJ4x6R2U5mM5X77Gw3bN3VcbjclS96MyKa6oZoMVk" />
  <meta name="baidu-site-verification" content="PpzM9WxOJU"/>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="description" content="人工智能的奇点，正在来临..."/>
  <meta name="keyword" content="AI"/>
  <link rel="shortcut icon" href="/img/avatar/tab.jpg"/>

  <!-- Place this tag in your head or just before your close body tag. -->
  <script async="async" defer="defer" src="https://buttons.github.io/buttons.js"></script>

  
    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/beantech.min.css"/>

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css"/>
    <link rel="stylesheet" href="/css/widget.css"/>
    <link rel="stylesheet" href="/css/rocket.css"/>
    <link rel="stylesheet" href="/css/signature.css"/>
    <link rel="stylesheet" href="/css/catalog.css"/>
    <link rel="stylesheet" href="/css/livemylife.css"/>

    
      <!-- wave start -->
      <link rel="stylesheet" href="/css/wave.css"/>
      <!-- wave end -->
    

    
      <!-- top start (article top hot config) -->
      <link rel="stylesheet" href="/css/top.css"/>
      <!-- top end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/css/scroll.css"/>
      <!-- ThemeColor end -->
    

    
      <!-- viewer start (Picture preview) -->
      <link rel="stylesheet" href="/css/viewer.min.css"/>
      <!-- viewer end -->
    

    
      <!-- Search start -->
      <link rel="stylesheet" href="/css/search.css"/>
      <!-- Search end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/css/themecolor.css"/>
      <!-- ThemeColor end -->
    

    

    
      <!-- gitalk start -->
      <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"> -->
      <link rel="stylesheet" href="/css/gitalk.css"/>
      <!-- gitalk end -->
    
  

  <!-- Custom Fonts -->
  <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
  <!-- Hux change font-awesome CDN to qiniu -->
  <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" type="text/css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <!-- Hux Delete, sad but pending in China <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'> <link
  href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/ css'> -->

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]-->

  <!-- ga & ba script hoook -->
  <link rel="canonical" href="https://11010101.xyz/cn/1.监督学习/">
  <title>
    
      第1章 统计学习方法概论 - Shawn Blog
    
  </title>
<meta name="generator" content="Hexo 5.4.0"></head>


<!-- hack iOS CSS :active style -->

	<body ontouchstart="" class="body--light body--light">


		<!-- ThemeColor -->
		
		<!-- ThemeColor -->
<style type="text/css">
  .body--light {
    --light-mode: none;
    --dark-mode: block;
  }
  .body--dark {
    --light-mode: block;
    --dark-mode: none;
  }
  i.mdui-icon.material-icons.light-mode {
    display: var(--light-mode);
  }
  i.mdui-icon.material-icons.dark-mode {
    display: var(--dark-mode);
  }
</style>
<div class="toggle" onclick="document.body.classList.toggle('body--dark')">
  <i class="mdui-icon material-icons light-mode"></i>
  <i class="mdui-icon material-icons dark-mode"></i>
</div>
<script>
  //getCookieValue
  function getCookieValue(a) {
    var b = document.cookie.match('(^|[^;]+)\\s*' + a + '\\s*=\\s*([^;]+)');
    return b
      ? b.pop()
      : '';
  }
  let themeMode = 'light';
  if (getCookieValue('sb-color-mode') && (getCookieValue('sb-color-mode') !== themeMode)) {
    let dbody = document.body.classList;
    themeMode === 'dark' ? dbody.remove('body--dark') : dbody.add('body--dark');
  }

  //setCookieValue
  var toggleBtn = document.querySelector(".toggle");
  toggleBtn.addEventListener("click", function () {
    var e = document.body.classList.contains("body--dark");
    var cookieString = e
      ? "dark"
      : "light";
    var exp = new Date();
    exp.setTime(exp.getTime() + 3 * 24 * 60 * 60 * 1000); //3天过期
    document.cookie = "sb-color-mode=" + cookieString + ";expires=" + exp.toGMTString() + ";path=/";
  });
</script>

		

		<!-- Gitter -->
		
		<!-- Gitter -->
<!-- Docs:https://gitter.im/?utm_source=left-menu-logo -->
<script>
  ((window.gitter = {}).chat = {}).options = {
    room: 'touch_fish/lie_down'
  };
</script>
<script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>

		

		<!-- Navigation (contains search)-->
		<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Shawn Blog</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <!-- Known Issue, found by Hux: <nav>'s height woule be hold on by its content. so, when navbar scale out, the <nav> will cover tags. also mask any touch event of tags, unfortunately. -->
    <div id="huxblog_navbar">
      <div class="navbar-collapse">
        <ul class="nav navbar-nav navbar-right">
          <li>
            <a href="/">主页</a>
          </li>

          
          
          
          
          <li>
            <a href="/about/">
              
              关于我
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/categories/">
              
              分类
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/archive/">
              
              归档
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/tags/">
              
              标签
              
              
            </a>
          </li>
          
          

          
          <li>
            <a class="popup-trigger">
              <span class="search-icon"></span>搜索</a>
          </li>
          

          <!-- LangSelect -->
          
          
          
          
          
        </ul>
      </div>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container -->
</nav>
<!-- progress -->
<div id="progress">
  <div class="line" style="width: 0%;"></div>
</div>

<script>
  // Drop Bootstarp low-performance Navbar Use customize navbar with high-quality material design animation in high-perf jank-free CSS3 implementation
  var $body = document.body;
  var $toggle = document.querySelector('.navbar-toggle');
  var $navbar = document.querySelector('#huxblog_navbar');
  var $collapse = document.querySelector('.navbar-collapse');

  $toggle.addEventListener('click', handleMagic)

  function handleMagic(e) {
    if ($navbar.className.indexOf('in') > 0) {
      // CLOSE
      $navbar.className = " ";
      // wait until animation end.
      setTimeout(function() {
        // prevent frequently toggle
        if ($navbar.className.indexOf('in') < 0) {
          $collapse.style.height = "0px"
        }
      }, 400)
    } else {
      // OPEN
      $collapse.style.height = "auto"
      $navbar.className += " in";
    }
  }
</script>


		<!-- Post Header (contains intro-header、signature、wordcount、busuanzi、waveoverlay) -->
		<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->

  <style type="text/css">
    .body--light {
      /* intro-header */
      --intro-header-background-image-url-home: url('/img/header_img/new_home_bg.jpg');
      --intro-header-background-image-url-post: url('');
      --intro-header-background-image-url-page: url('/img/header_img/archive_bg.jpg');
    }
    .body--dark {
      --intro-header-background-image-url-home: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/img/header_img/new_home_bg.jpg');
      --intro-header-background-image-url-post: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('');
      --intro-header-background-image-url-page: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/img/header_img/archive_bg.jpg');
    }

    header.intro-header {
       /*post*/
        background-image: var(--intro-header-background-image-url-post);
        /* background-image: url(''); */
      
    }

    
      #signature {/*signature*/
        background-image: url('/img/signature/my_signature.png');
      }
    
  </style>





<header class="intro-header">
  <!-- Signature -->
  <div id="signature">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
          
          <div class="post-heading">
            <div class="tags">
              
              <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
              
            </div>
            <h1>第1章 统计学习方法概论</h1>
            <h2 class="subheading">概论和有监督总结</h2>
            <span class="meta">
              Posted by 小于 on
              2022-03-17
            </span>


            
            <!-- WordCount start -->
            <div class="blank_box"></div>
            <span class="meta">
              Estimated Reading Time <span class="post-count">28</span> Minutes
            </span>
            <div class="blank_box"></div>
            <span class="meta">
              Words <span class="post-count">7k</span> In Total
            </span>
            <div class="blank_box"></div>
            <!-- WordCount end -->
            
            
            <!-- 不蒜子统计 start -->
            <span class="meta" id="busuanzi_container_page_pv">
              Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
            </span>
            <!-- 不蒜子统计 end -->
            


          </div>
          
        </div>
      </div>
    </div>
  </div>

  
  <!-- waveoverlay start -->
  <div class="preview-overlay">
    <svg class="preview-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
      <defs>
        <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path>
      </defs>
      <g class="preview-parallax">
        <use xlink:href="#gentle-wave" x="48" y="0" fill=var(--gentle-wave1)></use>
        <use xlink:href="#gentle-wave" x="48" y="3" fill=var(--gentle-wave2)></use>
        <use xlink:href="#gentle-wave" x="48" y="5" fill=var(--gentle-wave3)></use>
        <use xlink:href="#gentle-wave" x="48" y="7" fill=var(--gentle-wave)></use>
      </g>
    </svg>
  </div>
  <!-- waveoverlay end -->
  

</header>



		<!-- Main Content (Post contains
	Pager、
	tip、
	socialshare、
	gitalk、gitment、disqus-comment、
	Catalog、
	Sidebar、
	Featured-Tags、
	Friends Blog、
	anchorjs、
	) -->
		<!-- Modify by Yu-Hsuan Yen -->
<!-- Post Content -->
<article>
  <div class="container">
    <div class="row">
      <!-- Post Container -->
      <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1 post-container">

        <blockquote>
<p>统计学习方法第二版学习笔记和代码实现，搬运（bai piao）自大佬的github：<strong><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">https://github.com/fengdu78/lihang-code</a></strong>。个人做了部分增删来搭建整体学习框架，和理论丰富。侵删。</p>
</blockquote>
<h1 id="第1章-统计学习方法概论"><a href="#第1章-统计学习方法概论" class="headerlink" title="第1章 统计学习方法概论"></a>第1章 统计学习方法概论</h1><p>整合第一章和第十二章内容总结、附数学基础</p>
<p>统计学习方法：模型、算法、策略</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>监督学习过程中，模型就是所要学习的条件概率分布，还是决策函数</p>
<hr>
<h3 id="分类与标注问题"><a href="#分类与标注问题" class="headerlink" title="分类与标注问题"></a>分类与标注问题</h3><p>分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射。它们可以写成条件概率分布$P(Y|X)$，表示给定输入条件下输出的概率模型。也可以写成决策函数$Y=f(X)$的形式，表示输入到输出的非概率模型。</p>
<ul>
<li>朴素贝叶斯法、隐马尔可夫模型是概率模型；</li>
<li>感知机、$k$近邻法、支持向量机、提升方法是非概率模型；</li>
<li>决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看作是概率模型，又可以看作是非概率模型。</li>
</ul>
<h3 id="生成与判别模型"><a href="#生成与判别模型" class="headerlink" title="生成与判别模型"></a>生成与判别模型</h3><ul>
<li><p>判别模型：直接学习条件概率分布$P(Y|X)$或决策函数$Y=f(X)$的方法。对应的模型有感知机、$k$近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场。</p>
</li>
<li><p>生成模型：通过学习联合概率分布$P(X,Y)$，从而求得条件概率分布$P(Y|X)$的方法。对应的模型有：朴素贝叶斯法、隐马尔可夫模型。</p>
</li>
</ul>
<h3 id="特征空间"><a href="#特征空间" class="headerlink" title="特征空间"></a>特征空间</h3><ul>
<li>决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量。</li>
<li>感知机、支持向量机、k近邻法的特征空间是欧氏空间(更一般地，是希尔伯特空间)。</li>
<li>提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方法模型的特征空间。</li>
</ul>
<h3 id="线性与非线性"><a href="#线性与非线性" class="headerlink" title="线性与非线性"></a>线性与非线性</h3><ul>
<li>感知机模型是线性模型；而逻辑斯谛回归与最大熵模型、条件随机场是对数线性模型；</li>
<li>$k$近邻法、决策树、支持向量机(包含核函数)、提升方法使用的是非线性模型。</li>
</ul>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>策略就是定义什么样的损失，适配模型优化。</p>
<hr>
<h3 id="非概率模型"><a href="#非概率模型" class="headerlink" title="非概率模型"></a>非概率模型</h3><p>支持向量机、逻辑回归与最大熵模型、提升方法分别使用合页损失函数、逻辑斯谛损失函数、指数损失函数，如下：</p>
<script type="math/tex; mode=display">
[1-y f(x)]_{+}</script><script type="math/tex; mode=display">
\log[1+\exp (-y f(x))]</script><script type="math/tex; mode=display">
\exp (-y f(x))</script><p>学习的统一策略如下，</p>
<script type="math/tex; mode=display">
\min _{f \in H} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><p>第1项为经验风险(经验损失)，第2项为正则化项，$L(y,f(x))$为损失函数，$J(f)$为模型的复杂度，$\lambda \geq 0$为系数。</p>
<ul>
<li>支持向量机用$L_2$范数表示模型的复杂度；</li>
<li>原始的逻辑斯谛回归与最大熵模型没有正则化项，可以给它们加上$L_2$范数正则化项。</li>
<li>提升方法没有显式的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果。</li>
</ul>
<h3 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h3><p>概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计，学习的策略是极小化对数似然损失或极小化正则化的对数似然损失。对数似然损失可以写成：</p>
<script type="math/tex; mode=display">-logP(y|x)</script><p>极大后验概率估计时，正则化项是先验概率的负对数。</p>
<ul>
<li><p>决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正则化项是决策树的复杂度。</p>
</li>
<li><p>朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计，但这时模型含有隐变量。</p>
</li>
</ul>
<p>逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计(或正则化的极大似然估计)，又可以看成是极小化逻辑斯谛损失(或正则化的逻辑斯谛损失)。</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>模型的策略确定了具体的形式以后，算法就是最优化问题的方法。</p>
<hr>
<ul>
<li>朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即极大似然估计值</li>
<li>感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、拟牛顿法等一般的无约束最优化方法。</li>
<li>支持向量机学习，可以解凸二次规划的对偶问题。有序列最小最优化算法等方法。</li>
<li>决策树学习是基于启发式算法的典型例子。可以认为特征选择、生成、剪枝是启发式地进行正则化的极大似然估计。</li>
<li>提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点，启发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的。</li>
<li>EM算法是一种迭代的求解含隐变量概率模型参数的方法，它的收敛性可以保证，但是不能保证收敛到全局最优。</li>
</ul>
<p>支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优化问题，全局最优解保证存在。而其他学习问题则不是凸优化问题。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ol>
<li><p>对数损失-交叉熵取均值</p>
<ol>
<li><script type="math/tex; mode=display">J(\theta)=-\frac{1}{N}\sum_{i=1}^{N}[y_{i}log(h_{\theta}(x_i))+(1-y_{i})log(1-h_{\theta}(x_i))]</script></li>
<li><script type="math/tex; mode=display">CrossEntropy(\theta)=-\sum_{i=1}^{N}[y_{i}log(h_{\theta}(x_i))+(1-y_{i})log(1-h_{\theta}(x_i))]</script></li>
</ol>
</li>
<li><p>指数损失-Adaboost</p>
<script type="math/tex; mode=display">L(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}e^{-y_if(x_i)}</script></li>
<li><p>合页损失-SVM</p>
<script type="math/tex; mode=display">L(y·(w*x+b))=\sum_{i=1}^{N}[1-y(w*x+b)]_++\lambda||w||^2</script></li>
<li><p>决策树</p>
<script type="math/tex; mode=display">C_\alpha(T)=\sum_{t=1}^{T}N_tH_t(T)+\alpha|T|</script></li>
<li><p>focal loss- 推荐挖掘困难样本</p>
<script type="math/tex; mode=display">L_{fl}=-\alpha(1-p)^\gamma \log(p); p=1</script><script type="math/tex; mode=display">L_{fl}=-(1-\alpha)p^\gamma \log(1-p); p=0</script></li>
<li><p>多分类交叉熵损失</p>
</li>
<li>KL散度</li>
</ol>
<h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><ol>
<li><p>平方损失-回归</p>
<script type="math/tex; mode=display">MSE=\frac{1}{N}\sum_{i=1}^{N}(y_i^{pred}-y_i^{true})^2</script></li>
<li><p>绝对误差损失</p>
<script type="math/tex; mode=display">MAE=\sum_{i=1}^{n}|y-f(x)|</script></li>
<li><p>Huber损失<br>Huber损失结合了MSE和MAE的最佳特性。对于较小的误差，它是二次的，否则是线性的(对于其梯度也是如此)。Huber损失需要确定δ参数：</p>
</li>
</ol>
<p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/640.webp" alt></p>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><blockquote>
<blockquote>
<p>准确率，精确率，召回率，均方根误差</p>
</blockquote>
</blockquote>
<ul>
<li>准确率：正负样本不均衡时，大类别特别影响准确率；</li>
<li>精确率：$Precision=\frac{TP}{TP+FP}$</li>
<li>召回率：$Recall=\frac{TP}{TP+FN}$</li>
</ul>
<blockquote>
<blockquote>
<p>由此得到P-R曲线越右上角越好；且$F_1=\frac{2pr}{p+r}$</p>
</blockquote>
</blockquote>
<ul>
<li>均方根误差：异常点影响大<script type="math/tex; mode=display">RMSE=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(y_i^{pred}-y_i^{true})^2}</script></li>
<li>MAPE：平均绝对百分比误差，每个点误差进行归一化<script type="math/tex; mode=display">MAPE=\sum_{i=1}^{N}|\frac{y_i^{true}-y_i^{pred}}{y_i^{true}}|*\frac{100}{N}</script></li>
</ul>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><ul>
<li>优点：不受类分布的影响，适合与评估、比较类分布不平衡的数据集。</li>
<li>缺点：ROC和AUC仅适合于两类问题 ,对多类问题 ,无法直接应用。</li>
</ul>
<p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/AUC.gif" alt></p>
<h2 id="优化过拟合和欠拟合"><a href="#优化过拟合和欠拟合" class="headerlink" title="优化过拟合和欠拟合"></a>优化过拟合和欠拟合</h2><h3 id="降低过拟合"><a href="#降低过拟合" class="headerlink" title="降低过拟合"></a>降低过拟合</h3><ol>
<li>更多的数据；</li>
<li>降低模型复杂度；</li>
<li>正则化方法；</li>
<li>bagging 多个模型集成在一起，降低单一模型过拟合风险；</li>
<li>减少特征：PCA等<h3 id="降低欠拟合"><a href="#降低欠拟合" class="headerlink" title="降低欠拟合"></a>降低欠拟合</h3></li>
<li>添加新特征（深度学习提供稠密特征，因子分解机）</li>
<li>增加模型复杂度；</li>
<li>减少正则化系数</li>
</ol>
<h1 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wuliytTaotao/p/10513371.html">机器学习之数学基础01</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wuliytTaotao/p/10603576.html">机器学习之数学基础02</a></p>
<hr>
<h2 id="导数、偏导数与方向导数"><a href="#导数、偏导数与方向导数" class="headerlink" title="导数、偏导数与方向导数"></a>导数、偏导数与方向导数</h2><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/37.png" alt="avatar"></p>
<p>首先明确，导数是一个值，代表切线的斜率，而梯度是一个向量。<strong>最大方向导数的方向就是梯度代表的方向。</strong><br>梯度是$f(x)$对各个自变量$ x = [x_1, x_2, …, x_m]^\top$每一维分别求偏导数得到的向量。<br>从式（5）和（6）中我们也可以知道，当$d = \frac{\nabla f(x)}{|\nabla f(x)|}$方向导数最大。 <strong>最大方向导数的方向就是梯度的方向，最大的方向导数就是梯度的欧几里德范数。</strong></p>
<h2 id="机器学习中的梯度下降"><a href="#机器学习中的梯度下降" class="headerlink" title="机器学习中的梯度下降"></a>机器学习中的梯度下降</h2><p>梯度下降首先需要明确损失函数Loss function（一个样本损失）和代价函数Cost function（统计平均意义下的损失）。</p>
<script type="math/tex; mode=display">w^{(k+1)} = w^{(k)} - \alpha \cdot  \nabla C(w^{(k)})</script><p>f(x,y)=x²+y²<br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/tidu1.png" alt="avatar"><br>将f(x,y)=x²+y²看做<strong>二维等高图</strong><br><strong>对于二维曲面来说，切线的方向有上有下描述的是函数值的方向；梯度代表使函数值增大时，各个自变量的变化方向；梯度下降就是梯度反方向更新各个自变量可以使得函数值减小。梯度方向与切线方向垂直。</strong></p>
<p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/tidu2.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/futidu.jpeg" alt="avatar"></p>
<h3 id="1-1、批梯度下降比较"><a href="#1-1、批梯度下降比较" class="headerlink" title="1.1、批梯度下降比较"></a>1.1、批梯度下降比较</h3><ol>
<li>批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</li>
<li>随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。</li>
<li>小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</li>
</ol>
<blockquote>
<blockquote>
<p><strong>全局最优</strong>&lt;=&gt;通过梯度下降得到的最小值在全局Loss中也是最小值&lt;=&gt;Loss函数是否是凸的&lt;=&gt;<strong>二阶导(海森矩阵)判断是否正定</strong></p>
</blockquote>
</blockquote>
<h3 id="1-2、最速下降"><a href="#1-2、最速下降" class="headerlink" title="1.2、最速下降"></a>1.2、最速下降</h3><p>最速下降法（Steepest descent）是梯度下降法的一种更具体实现形式，其理念为在每次迭代中选择合适的步长$\alpha_k$使得目标函数值能够得到最大程度的减少。<br>每一次迭代，沿梯度的反方向，我们总可以找到一个</p>
<script type="math/tex; mode=display">w^{(k+1)} = w^{(k)} - \alpha_k \cdot  \nabla C(w^{(k)})</script><p>使得在这个方向上$f(x^{(k+1)})$取最小值。</p>
<script type="math/tex; mode=display">\alpha_k = \mathop{\arg\min}_{\alpha \ge 0} f(x^{(k)} - \alpha \nabla f(x^{(k)}))</script><p>有意思的是，最速下降法每次更新的轨迹都和上一次垂直。而且只要梯度$\nabla f(x^{(k)}) \not = 0$则$f(x^{(k+1)}) &lt; f(x^{(k)})$<br>（即梯度不等于 0 时，肯定会下降。）</p>
<h3 id="1-3、多分类梯度下降"><a href="#1-3、多分类梯度下降" class="headerlink" title="1.3、多分类梯度下降"></a>1.3、多分类梯度下降</h3><p>softmax 函数的表达式为:</p>
<script type="math/tex; mode=display">y_i = \frac{e^{z_i}}{\sum_{t = 1}^m e^{z_t}}</script><script type="math/tex; mode=display">\frac{\partial y_i}{\partial z_j}
= \frac{\partial \frac{e^{z_i}}{\sum_{t = 1}^m e^{z_t}}}{\partial z_j}
\tag{2}</script><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38.jpg" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39.jpg" alt="avatar"></p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>在确定搜索方向时，<strong>梯度下降和最速下降只用到了目标函数的一阶导数（梯度），而牛顿法（Newton’s method）用到了二阶（偏）导数。</strong><br>牛顿法的基本思路是在每次迭代中，利用二次型函数来局部近似目标函数f，并求解近似函数的极小点作为下一个迭代点，牛顿法自变量x的更新公式为：</p>
<script type="math/tex; mode=display">x^{(k+1)} = x^{(k)} - F(x^{(k)})^{-1}\nabla f( x^{(k)})</script><p>当前点离min点较近趋势可能不会错，但较远时会带偏。</p>
<h3 id="Levenberg-Marquardt修正"><a href="#Levenberg-Marquardt修正" class="headerlink" title="Levenberg-Marquardt修正"></a>Levenberg-Marquardt修正</h3><ul>
<li>牛顿法引入二阶导数拟合f，依据f的变化趋势去梯度下降，收敛更快，迭代次数更少。但同时会引入问题，拟合会有误差，当前离min点较远时，趋势可能不对。</li>
<li>多元函数的Hessian矩阵就类似一元函数的二阶导。多元函数Hessian矩阵半正定就相当于一元函数二阶导非负，半负定就相当于一元函数二阶导非正</li>
</ul>
<p>如果黑塞矩阵$F(x^{(k)})$不正定，那么搜索方向$ d^{(k)} = - F( x^{(k)})^{-1}\nabla f( x^{(k)})$可能不会是下降方向。 牛顿法的 Levenberg-Marquardt 修正可以解决这个问题：</p>
<script type="math/tex; mode=display">x^{(k+1)} = x^{(k)} - \alpha_k(F( x^{(k)})  + \mu_k I)^{-1}\nabla f(x^{(k)})</script><p>其中，$\mu_k \ge 0$，I为单位矩阵。在该修正中，$F( x^{(k)})$可以不正定，但是$ G = F( x^{(k)}) + \mu_k , I$需要是正定的，所以，取适当的$\mu_k$使得$ G$正定即可。（矩阵正定，当前仅当所有特征值大于 0。）</p>
<h3 id="牛顿法-vs-梯度下降"><a href="#牛顿法-vs-梯度下降" class="headerlink" title="牛顿法 vs 梯度下降"></a>牛顿法 vs 梯度下降</h3><ul>
<li>牛顿法是二阶收敛，梯度下降法是一阶收敛，所以牛顿法就更快。</li>
<li>更通俗地，梯度下降法只从当前位置选择一个坡度最大的方向走一步，而牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑走了一步后，坡度是否会变得更大。</li>
<li>从几何上说，牛顿法就是用一个二次曲面去拟合当前位置的的局部曲面，而梯度下降法用的是一个平面去拟合，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</li>
</ul>
<h3 id="拟牛顿法-牛顿法hessian矩阵的优化求解"><a href="#拟牛顿法-牛顿法hessian矩阵的优化求解" class="headerlink" title="拟牛顿法-牛顿法hessian矩阵的优化求解"></a>拟牛顿法-牛顿法hessian矩阵的优化求解</h3><p>在每次迭代的时候计算一个矩阵，其逼近海塞矩阵的逆。最重要的是，该逼近值只是使用损失函数的一阶偏导来计算求出$  H_{k+1}$给出 $  H_{k}$，梯度$f(  x^{(k)})$，$  d^{(k)}$，$\alpha_k$找到 $  H_{k+1}$的递推式，那么在迭代过程中就不需要涉及到黑塞矩阵也不会求逆。</p>
<script type="math/tex; mode=display">\boldsymbol{H}_{k+1}=\boldsymbol{H}_{k}+\frac{\left(\Delta \boldsymbol{x}^{(k)}-\boldsymbol{H}_{k} \Delta \boldsymbol{g}^{(k)}\right)\left(\Delta \boldsymbol{x}^{(k)}-\boldsymbol{H}_{k} \Delta \boldsymbol{g}^{(k)}\right)^{\top}}{\Delta \boldsymbol{g}^{(k) \top}\left(\Delta \boldsymbol{x}^{(k)}-\boldsymbol{H}_{k} \Delta \boldsymbol{g}^{(k)}\right)}</script><p>$\Delta x^{(k)}=\alpha_{k} d^{(k)}$，$\Delta \boldsymbol{g}^{(k)}=\boldsymbol{g}^{(k+1)}-\boldsymbol{g}^{(k)}$<br>$  H_0$可以取任一对称正定实矩阵。</p>
<h2 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h2><p><a target="_blank" rel="noopener" href="http://liuchengxu.org/blog-cn/posts/bias-variance/">参考链接</a></p>
<ol>
<li>泛化误差：以回归任务为例, 学习算法的平方预测误差期望为:<script type="math/tex; mode=display">Err(\mathbf{x}) = E\left[\left( y - f(\mathbf{x}; D) \right)^2\right]</script></li>
<li>方差：在一个训练集 D上模型 f 对测试样本 x 的预测输出为 f(x;D), 那么学习算法 f 对测试样本 x 的 期望预测和使用样本数相同的不同训练集产生的方差为:<script type="math/tex; mode=display">\overline{f}(\mathbf{x}) = E_D\left[f\left(\mathbf{x}; D\right)\right]</script><script type="math/tex; mode=display">var(\mathbf{x}) = E_D\left[\left( f(\mathbf{x}; D) - \overline{f}(\mathbf{x}) \right)^2\right]</script></li>
<li>偏差：期望预测与真实标记的误差称为偏差(bias), 为了方便起见, 我们直接取偏差的平方:<script type="math/tex; mode=display">bias^2(\mathbf{x}) = \left( \overline{f}(\mathbf{x}) - y \right)^2</script></li>
<li>噪声：噪声为真实标记与数据集中的实际标记间的偏差:<script type="math/tex; mode=display">\epsilon^2 = E_D\left[ (y_D - y)^2 \right]</script></li>
</ol>
<h3 id="方差与偏差推导"><a href="#方差与偏差推导" class="headerlink" title="方差与偏差推导"></a>方差与偏差推导</h3><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39.png" alt="avatar"></p>
<ul>
<li>偏差度量了学习算法的期望预测与真实结果的偏离程序, 即 刻画了学习算法本身的拟合能力 .</li>
<li>方差度量了同样大小的训练集的变动所导致的学习性能的变化, 即 刻画了数据扰动所造成的影响 .</li>
<li>噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界, 即 刻画了学习问题本身的难度.</li>
</ul>
<p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/40.png" alt="avatar"></p>
<h2 id="L1和L2正则"><a href="#L1和L2正则" class="headerlink" title="L1和L2正则"></a>L1和L2正则</h2><ul>
<li>L1大部分特征的权重降为0，变稀疏矩阵，只保留少量特征。目标函数等高线与L1正则的解空间决定，“棱”状更能碰撞出稀疏值。类似减少特征的方式解决过拟合</li>
<li>L2少量降为0，大部分权重减小。降低所有特征的权重。整体减小所有特征权重，来解决过拟合</li>
</ul>
<p>L0范数指向量中非零元素的个数</p>
<script type="math/tex; mode=display">\|w\|_0=\sum_{w_i!=0}^{W}{|w_i|}</script><p>L1范数：向量中每个元素绝对值的和</p>
<script type="math/tex; mode=display">\|w\|_1=\sum_{i=1}^{N}{|w_i|}</script><p>L2范数：向量元素绝对值的平方和再开平方</p>
<script type="math/tex; mode=display">\|w\|_2=\sqrt{\sum_{i=1}^{N}{w_i^2}}</script><p>L1正则公式</p>
<script type="math/tex; mode=display">L=L_{0}+\lambda\sum_j|w_j|</script><p>L2正则公式</p>
<script type="math/tex; mode=display">L=L_{0}+\lambda\sum_jw_j^2</script><h3 id="解的稀疏性"><a href="#解的稀疏性" class="headerlink" title="解的稀疏性"></a>解的稀疏性</h3><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/l1l2.png" alt="avatar"></p>
<h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><h4 id="极大似然与概率"><a href="#极大似然与概率" class="headerlink" title="极大似然与概率"></a>极大似然与概率</h4><ul>
<li>1&gt; 多个时间概率相乘得到似然表达式，值的大小意味着这组样本值都发生的可能性的大小；</li>
<li>2&gt; 对似然表达式求导，必要时进行预处理，比如取对数（逻辑回归需要），令其导数为0，得到似然方程。</li>
<li>3&gt; 求解似然方程，得到的参数解即为极大似然估计的解。<blockquote>
<blockquote>
<p>何为极大似然：“模型已定，参数未知”。通过若干次试验，观察其结果，反推得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。<strong>利用已知的样本结果，反推取得所有样本最大概率的参数值。</strong></p>
</blockquote>
</blockquote>
</li>
</ul>
<h4 id="概率分布"><a href="#概率分布" class="headerlink" title="概率分布"></a>概率分布</h4><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/bern.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/er.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/gauss.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/junyun.png" alt="avatar"></p>
<h4 id="条件概率与链式法则"><a href="#条件概率与链式法则" class="headerlink" title="条件概率与链式法则"></a>条件概率与链式法则</h4><ol>
<li>协方差矩阵<script type="math/tex; mode=display">cov(X,Y)= E\{(X_i-E(X))(Y_i-E(Y)\},i=1,2,……n;j=i,2,……n</script></li>
<li>全概率公式<script type="math/tex; mode=display">P(X=x)=\sum_{k=1}^{K}P(X=x|Y=c_k)P(Y=c_k)</script></li>
<li>朴素贝叶斯=贝叶斯公式+条件独立性假设<ol>
<li>贝叶斯公式<script type="math/tex; mode=display">y=argmax_{c_k}\frac{P(Y=c_k)P(X=x|Y=c_k)}{\displaystyle\sum_{k}^KP(Y=c_k)P(X=x|Y=c_k)}</script></li>
<li>条件独立性假设<script type="math/tex; mode=display">P(X=x|Y=c_k)=\prod_{j=1}^nP(X_j=x_j|Y=c_k)P(Y=c_k)</script></li>
</ol>
</li>
</ol>
<blockquote>
<blockquote>
<p>计算后验概率分布，将后验概率最大的x的类别作为输出，使用贝叶斯定理计算最大后验概率。另：贝叶斯公式=条件概率公式+全概率公式。</p>
</blockquote>
</blockquote>
<h4 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h4><p> 如果$P(x_i|y=c_k)$中的某一项为0，则其联合概率的乘积也为0.如果是k类分母+k，分子+1；分母加k的原因是使之满足全概率公式。</p>
<h4 id="置信度与置信区间"><a href="#置信度与置信区间" class="headerlink" title="置信度与置信区间"></a>置信度与置信区间</h4><p>置信度：95%，置信区间(u-a,u+a)样本数目不变的情况下，做100次试验，有95个置信区间包含了总体真值(虚线)。</p>
<script type="math/tex; mode=display">P(\mu - 1.96 \frac{\sigma}{\sqrt{n}} < M < \mu + 1.96 \frac{\sigma}{\sqrt{n}} ) = 0.95</script><p>样本均值为整体均值的一个点估计。<strong>置信水平95%表示，有100个样本，每个样本会计算出一个置信区间。其中有95个置信区间包含了总体均值。也就是包含总体平均值的概率为95%。样本估计总体—核心思想</strong><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/zhixindu.gif" alt="avatar"></p>
<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><ul>
<li><p>信息熵：</p>
<script type="math/tex; mode=display">H(D)=-\sum_{y=k}^{K}p_k·log(p_k)</script></li>
<li><p>联合熵：</p>
<script type="math/tex; mode=display">H(X,Y)=-\sum_{x,y}p(x,y)\log p(x,y)</script></li>
<li><p>条件熵：条件熵=联合熵-熵</p>
<script type="math/tex; mode=display">H(Y|X)=H(X,Y)-H(X)</script><script type="math/tex; mode=display">H(D|f)=\sum_{v\epsilon V_f}\frac{|D_v|}{|D|}H(D_v)</script></li>
<li>信息增益：<script type="math/tex; mode=display">Gain(D,f)=H(D)-H(D|f)</script></li>
<li>交叉熵：<script type="math/tex; mode=display">H(p,q)=-\sum_x p(x)\log q(x)</script></li>
<li>KL散度（相对熵）=交叉熵-熵<script type="math/tex; mode=display">D_{KL}(p||q)=H(p,q)-H(p)</script><blockquote>
<blockquote>
<p>p对q的相对熵,q拟合p的分布情况</p>
<script type="math/tex; mode=display">D_{KL}(p||q)=\sum_{x}p(x)\log \frac{p(x)}{q(x)}=-\sum_x p(x)\log q(x)-(-\sum_x p(x)\log p(x))</script><p>相对熵可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 p 与 q 之间的对数差在 p 上的期望值。</p>
</blockquote>
</blockquote>
</li>
</ul>
<h2 id="矩阵论"><a href="#矩阵论" class="headerlink" title="矩阵论"></a>矩阵论</h2><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/Matrix.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/Matrix2.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/Matrix3.png" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/Matrix4.png" alt="avatar"></p>
<h3 id="向量与矩阵求导"><a href="#向量与矩阵求导" class="headerlink" title="向量与矩阵求导"></a>向量与矩阵求导</h3><ul>
<li>包含元素x的求导就是元素x对矩阵的各元素a求导,或者矩阵各元素a对元素x的求导。</li>
<li><p>向量对向量求导</p>
<ol>
<li>行向量m对列向量n：行的每一个元素对列向量求导，导数矩阵：n*m;</li>
<li>列向量m对行向量n：列的每一个元素对行向量求导，导数矩阵：m*n;</li>
<li>行向量m对行向量n：一行m*n列</li>
<li>列向量m对列向量n：一列m*n行</li>
</ol>
</li>
<li><p>矩阵对向量求导</p>
<ol>
<li>矩阵看做列向量，对行向量求导：<strong>矩阵</strong>对各个元素求导，横向拼接</li>
<li>矩阵看做列向量，对列向量求导：<strong>各个元素</strong>对列向量求导，整体拼接</li>
</ol>
</li>
<li><p>向量对矩阵求导</p>
<ol>
<li>行向量对矩阵求导：行向量对矩阵的s<strong>各个元素</strong>求导，然后拼接</li>
<li>列向量对矩阵求导：列向量的<strong>各个元素</strong>对矩阵求导，然后拼接</li>
</ol>
</li>
<li><p>矩阵Y对矩阵X求导</p>
<ol>
<li>Y看做列向量，X看做行向量：Y的列向量对X的行向量逐一求导</li>
</ol>
</li>
</ul>
<h3 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h3><h4 id="2-1-矩阵分解为特征值与特征向量"><a href="#2-1-矩阵分解为特征值与特征向量" class="headerlink" title="2.1 矩阵分解为特征值与特征向量"></a>2.1 矩阵分解为特征值与特征向量</h4><script type="math/tex; mode=display">Av=\lambda v</script><p>$V=[v_1,v_2,v_3,,,,v_n]$,$\Lambda=[\lambda_1,\lambda_2,\lambda_3,,,]$则有</p>
<blockquote>
<blockquote>
<p>$\lambda$降序排列</p>
<script type="math/tex; mode=display">A=V * diag(\lambda)*V^{-1}</script><p>正定矩阵的转置就是正定矩阵的逆</p>
<script type="math/tex; mode=display">A=Q \Lambda Q^{T}</script><p>Q是正交矩阵，将A看作沿方向$v^i$延展$\lambda ^i$倍的空间。所有的特征值都是正数则正定，非负数半正定。<strong>但是仅对于方阵才行</strong></p>
</blockquote>
</blockquote>
<h4 id="矩阵分解为奇异值与奇异向量SVD"><a href="#矩阵分解为奇异值与奇异向量SVD" class="headerlink" title="矩阵分解为奇异值与奇异向量SVD"></a>矩阵分解为奇异值与奇异向量SVD</h4><script type="math/tex; mode=display">A_{m,n}=U_{m,m}D_{m,n}V^T_{n,n}</script><p>U,V正交矩阵，D对角矩阵，对角线元素为为奇异值，U,V列向量为奇异向量<br>U是$AA^T$的特征向量；V是$A^TA$的特征向量。$AA^T$特征值的平方根就是D的非0奇异值<strong>只有非方阵，才行</strong></p>
<h2 id="拉格朗日乘法"><a href="#拉格朗日乘法" class="headerlink" title="拉格朗日乘法"></a>拉格朗日乘法</h2><script type="math/tex; mode=display">\begin{aligned}
&{}\max f(x, y), \\
&{}\text{s.t. } g(x, y) = c.
\end{aligned}</script><script type="math/tex; mode=display">\mathcal{L}(x, y, \lambda) \overset{\text{def}}{=}f(x, y) + \lambda\cdot g(x, y).</script><script type="math/tex; mode=display">\begin{cases}
\frac{\partial \mathcal{L}}{\partial x} = 0, \\
\frac{\partial \mathcal{L}}{\partial y} = 0, \\
\frac{\partial \mathcal{L}}{\partial \lambda} = 0.
\end{cases}</script><p>函数 f(x,y)=x²+y² 是曲面<strong>特别地，对于序列 {d1,d2,…} 来说，f(x,y)=dk 形成了一系列的曲线。若将 dk 理解为高度，则这一系列的曲线即是函数 f(x,y) 的等高线组。</strong> 同样，对于约束 g(x,y)=x²-y=C 来说它也是一条曲线，我们称之为约束曲线。对g曲线求导得到法向量，对f也如此；故有下式成立</p>
<script type="math/tex; mode=display">\Bigl(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\Bigr) = \lambda\Bigl(\frac{\partial g}{\partial x}, \frac{\partial g}{\partial y}\Bigr)</script><p>函数曲线相切，意味着两个函数的法线在切点重合，也就是两个函数的法向量相差一个系数 λ，这也就是说两个函数在切点的梯度向量相差一个系数 λ</p>
<h3 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h3><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/kkt.jpeg" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/lglr.jpg" alt="avatar"><br>因此我们说，拉格朗日乘数法有很直观的物理意义。<br>对于h(x)&gt;=0的设定如下<br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/hx1.jpg" alt="avatar"><br><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/hx2.jpg" alt="avatar"></p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>高斯于1823年在误差$e_1,…,e_n$独立同分布的假定下,证明了最小二乘方法的一个最优性质: 在所有无偏的线性估计类中,最小二乘方法是其中方差最小的！<br>对于数据$(x_i, y_i)   (i=1, 2, 3…,m)$</p>
<p>拟合出函数$h(x)$</p>
<p>有误差，即残差：$r_i=h(x_i)-y_i$</p>
<p>此时$L2$范数(残差平方和)最小时，$h(x)$ 和 $y$ 相似度最高，更拟合一般的$H(x)$为$n$次的多项式，$H(x)=w_0+w_1x+w_2x^2+…w_nx^n$</p>
<p>$w(w_0,w_1,w_2,…,w_n)$为参数</p>
<p>最小二乘法就是要找到一组 $w(w_0,w_1,w_2,…,w_n)$ ，使得$\sum_{i=1}^n(h(x_i)-y_i)^2$ (残差平方和) 最小</p>
<p>即，求 $min\sum_{i=1}^n(h(x_i)-y_i)^2$</p>
<p>举例：我们用目标函数$y=sin2{\pi}x$, 加上一个正态分布的噪音干扰，用多项式去拟合【例1.1 11页】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> leastsq</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<ul>
<li>ps: numpy.poly1d([1,2,3])  生成  $1x^2+2x^1+3x^0$*</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目标函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">real_func</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.sin(<span class="number">2</span>*np.pi*x)</span><br><span class="line"><span class="comment"># 多项式model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_func</span>(<span class="params">p, x</span>):</span></span><br><span class="line">    f = np.poly1d(p)</span><br><span class="line">    <span class="keyword">return</span> f(x)</span><br><span class="line"><span class="comment"># 优化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residuals_func</span>(<span class="params">p, x, y</span>):</span></span><br><span class="line">    ret = fit_func(p, x) - y</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">x_points = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1000</span>) <span class="comment"># 加上正态分布噪音的目标函数的值</span></span><br><span class="line">y_ = real_func(x)</span><br><span class="line">y = [np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>) + y1 <span class="keyword">for</span> y1 <span class="keyword">in</span> y_]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fitting</span>(<span class="params">M=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    M    为 多项式的次数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p_init = np.random.rand(M + <span class="number">1</span>)</span><br><span class="line">    p_lsq = leastsq(residuals_func, p_init, args=(x, y))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Fitting Parameters:&#x27;</span>, p_lsq[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 可视化</span></span><br><span class="line">    plt.plot(x_points, real_func(x_points), label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">    plt.plot(x_points, fit_func(p_lsq[<span class="number">0</span>], x_points), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line">    plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    <span class="keyword">return</span> p_lsq</span><br></pre></td></tr></table></figure>
<p>以下为通过调整拟合函数的复杂度，观察对曲线的拟合效果</p>
<h3 id="M-0"><a href="#M-0" class="headerlink" title="M=0"></a>M=0</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p_lsq_0 = fitting(M=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Fitting Parameters: [0.023975]
</code></pre><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_12_1.png" alt="png"></p>
<h3 id="M-1"><a href="#M-1" class="headerlink" title="M=1"></a>M=1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p_lsq_1 = fitting(M=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Fitting Parameters: [-1.22711987  0.63753492]
</code></pre><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_14_1.png" alt="png"></p>
<h3 id="M-9"><a href="#M-9" class="headerlink" title="M=9"></a>M=9</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p_lsq_9 = fitting(M=<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Fitting Parameters: [ 2.27009559e+04 -9.75889799e+04  1.75711812e+05 -1.72351120e+05
  1.00239965e+05 -3.51721425e+04  7.22377724e+03 -8.09192861e+02
  4.51301228e+01 -2.60185475e-01]
</code></pre><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_16_1.png" alt="png"></p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>结果显示过拟合， 引入正则化项(regularizer)</p>
<p>$Q(x)=\sum_{i=1}^n(h(x_i)-y_i)^2+\lambda||w||^2$。</p>
<p>正则化可以是参数向量的L2范数,也可以是L1范数。</p>
<ul>
<li><p>L1: regularization*abs(p)</p>
</li>
<li><p>L2: 0.5 * regularization * np.square(p)</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">regularization = <span class="number">0.001</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residuals_func_regularization</span>(<span class="params">p, x, y</span>):</span></span><br><span class="line">    ret = fit_func(p, x) - y</span><br><span class="line">    ret = np.append(ret,</span><br><span class="line">                    np.sqrt(<span class="number">0.5</span> * regularization * np.square(p)))  <span class="comment"># L2范数作为正则化项</span></span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p_init = np.random.rand(<span class="number">9</span> + <span class="number">1</span>)</span><br><span class="line">p_lsq_regularization = leastsq(</span><br><span class="line">    residuals_func_regularization, p_init, args=(x, y))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x_points, real_func(x_points), label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">plt.plot(x_points, fit_func(p_lsq_9[<span class="number">0</span>], x_points), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line">plt.plot(</span><br><span class="line">    x_points,</span><br><span class="line">    fit_func(p_lsq_regularization[<span class="number">0</span>], x_points),</span><br><span class="line">    label=<span class="string">&#x27;regularization&#x27;</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x7efdf98a4950&gt;
</code></pre><p><img src="/cn/1.%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_21_1.png" alt="png"></p>
<p>是不是发现，变动还是很明显的。。。</p>
<p>伯努利模型的极大似然估计以及贝叶斯估计中的<strong>统计学习方法三要素</strong>如下：  </p>
<ol>
<li><strong>极大似然估计</strong><br><strong>模型：</strong> $\mathcal{F}=\{f|f_p(x)=p^x(1-p)^{(1-x)}\}$<br><strong>策略：</strong> 最大化似然函数<br><strong>算法：</strong> $\displaystyle \mathop{\arg\min}_{p} L(p)= \mathop{\arg\min}_{p} \binom{n}{k}p^k(1-p)^{(n-k)}$</li>
<li><strong>贝叶斯估计</strong><br><strong>模型：</strong> $\mathcal{F}=\{f|f_p(x)=p^x(1-p)^{(1-x)}\}$<br><strong>策略：</strong> 求参数期望<br><strong>算法：</strong><script type="math/tex; mode=display">\begin{aligned}  E_\pi\big[p \big| y_1,\cdots,y_n\big]
& = {\int_0^1}p\pi (p|y_1,\cdots,y_n) dp \\
& = {\int_0^1} p\frac{f_D(y_1,\cdots,y_n|p)\pi(p)}{\int_{\Omega}f_D(y_1,\cdots,y_n|p)\pi(p)dp}dp \\
& = {\int_0^1}\frac{p^{k+1}(1-p)^{(n-k)}}{\int_0^1 p^k(1-p)^{(n-k)}dp}dp
\end{aligned}</script></li>
</ol>
<p><strong>伯努利模型的极大似然估计：</strong><br>定义$P(Y=1)$概率为$p$，可得似然函数为：<script type="math/tex">L(p)=f_D(y_1,y_2,\cdots,y_n|\theta)=\binom{n}{k}p^k(1-p)^{(n-k)}</script>方程两边同时对$p$求导，则：<script type="math/tex">\begin{aligned}
0 & = \binom{n}{k}[kp^{k-1}(1-p)^{(n-k)}-(n-k)p^k(1-p)^{(n-k-1)}]\\
& = \binom{n}{k}[p^{(k-1)}(1-p)^{(n-k-1)}(m-kp)]
\end{aligned}</script>可解出$p$的值为$p=0,p=1,p=k/n$，显然$\displaystyle P(Y=1)=p=\frac{k}{n}$  </p>
<p><strong>伯努利模型的贝叶斯估计：</strong><br>定义$P(Y=1)$概率为$p$，$p$在$[0,1]$之间的取值是等概率的，因此先验概率密度函数$\pi(p) = 1$，可得似然函数为： <script type="math/tex">L(p)=f_D(y_1,y_2,\cdots,y_n|\theta)=\binom{n}{k}p^k(1-p)^{(n-k)}</script><br>根据似然函数和先验概率密度函数，可以求解$p$的条件概率密度函数：<script type="math/tex">\begin{aligned}\pi(p|y_1,\cdots,y_n)&=\frac{f_D(y_1,\cdots,y_n|p)\pi(p)}{\int_{\Omega}f_D(y_1,\cdots,y_n|p)\pi(p)dp}\\
&=\frac{p^k(1-p)^{(n-k)}}{\int_0^1p^k(1-p)^{(n-k)}dp}\\
&=\frac{p^k(1-p)^{(n-k)}}{B(k+1,n-k+1)}
\end{aligned}</script>所以$p$的期望为：<script type="math/tex">\begin{aligned}
E_\pi[p|y_1,\cdots,y_n]&={\int}p\pi(p|y_1,\cdots,y_n)dp \\
& = {\int_0^1}\frac{p^{(k+1)}(1-p)^{(n-k)}}{B(k+1,n-k+1)}dp \\
& = \frac{B(k+2,n-k+1)}{B(k+1,n-k+1)}\\
& = \frac{k+1}{n+2}
\end{aligned}</script><br>$\therefore \displaystyle P(Y=1)=\frac{k+1}{n+2}$</p>


        <hr>
        <!-- Pager -->
        <ul class="pager">
          
          <li class="previous">
            <a href="/cn/2.Perception/" data-toggle="tooltip" data-placement="top" title="第2章 感知机">&larr; Previous Post</a>
          </li>
          
          
          <li class="next">
            <a href="/cn/LeetCode_动态规划/" data-toggle="tooltip" data-placement="top" title="LeetCode_动态规划">Next Post &rarr;</a>
          </li>
          
        </ul>

        
        <!-- tip start -->
        <!-- tip -->
<!-- tip start -->
<div class="tip">
  <p>
    
      如果您喜欢此博客或发现它对您有用，则欢迎对此发表评论。 也欢迎您共享此博客，以便更多人可以参与。 如果博客中使用的图像侵犯了您的版权，请与作者联系以将其删除。 谢谢 ！
    
  </p>
</div>
<!-- tip end -->

        <!-- tip end -->
        

        
        <!-- Sharing Srtart -->
        <!-- Social Social Share Post -->
<!-- Docs:https://github.com/overtrue/share.js -->

<div class="social-share" data-initialized="true" data-disabled="tencent ,douban ,qzone ,linkedin ,facebook ,google ,diandian" data-wechat-qrcode-helper="" align="center">
  <ul class="list-inline text-center social-share-ul">
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-twitter">
        <i class="fa fa-twitter fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a class="social-share-icon icon-wechat">
        <i class="fa fa-weixin fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-weibo">
        <i class="fa fa-weibo fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-qq">
        <i class="fa fa-qq fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon" href="mailto:?subject=第1章 统计学习方法概论&body=Hi,I found this website and thought you might like it https://11010101.xyz/cn/1.监督学习/">
        <i class="fa fa-envelope fa-1x" aria-hidden="true"></i>
      </a>
    </li>
  </ul>
</div>

<!-- css & js -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css"> -->
<script defer="defer" async="true" src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>

        <!-- Sharing End -->
        
        <hr>

        <!-- comments start -->
        <!-- 1. gitalk comment -->

  <!-- gitalk start -->
  <!-- Docs:https://github.com/gitalk/gitalk/blob/master/readme-cn.md -->

  <div id="gitalk-container"></div>

  
    <!-- <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.js"></script> -->
    <script src="/js/comment/gitalk.js"></script>
  

  <script>
    var gitalk = new Gitalk({
      clientID: 'a9a645b881c78d12baa8',
      clientSecret: '144741a3fa325fe22d7207d5698374724cd412b7',
      repo: 'Muzhi1920.github.io',
      owner: 'Muzhi1920',
      admin: 'Muzhi1920',
      id: 'Thu Mar 17 2022 10:34:17 GMT+0000 | truncate: 50', // Ensure uniqueness and length less than 50
      distractionFreeMode: false, // Facebook-like distraction free mode
      perPage: 10,
      pagerDirection: 'last',
      createIssueManually: false,
      language: 'zh-CN',
      proxy: 'https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token'
    });
    gitalk.render('gitalk-container');

    var gtFolded = () => {
      setTimeout(function () {
        let markdownBody = document.getElementsByClassName("markdown-body");
        let list = Array.from(markdownBody);
        list.forEach(item => {
          if (item.clientHeight > 250) {
            item.classList.add('gt-comment-body-folded');
            item.style.maxHeight = '250px';
            item.title = 'Click to Expand';
            item.onclick = function () {
              item.classList.remove('gt-comment-body-folded');
              item.style.maxHeight = '';
              item.title = '';
              item.onclick = null;
            };
          }
        })
      }, 800);
    }
  </script>

  <!-- gitalk end -->


<!-- 2. gitment comment -->


<!-- 3. disqus comment -->


        <!-- comments end -->
        <hr>

      </div>

      <!-- Catalog: Tabe of Content -->
      <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">目录</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E7%AC%AC1%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">第1章 统计学习方法概论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text">模型</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%88%86%E7%B1%BB%E4%B8%8E%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98"><span class="toc-nav-number">1.1.1.</span> <span class="toc-nav-text">分类与标注问题</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%94%9F%E6%88%90%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B"><span class="toc-nav-number">1.1.2.</span> <span class="toc-nav-text">生成与判别模型</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4"><span class="toc-nav-number">1.1.3.</span> <span class="toc-nav-text">特征空间</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%BA%BF%E6%80%A7%E4%B8%8E%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="toc-nav-number">1.1.4.</span> <span class="toc-nav-text">线性与非线性</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E7%AD%96%E7%95%A5"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text">策略</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E9%9D%9E%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B"><span class="toc-nav-number">1.2.1.</span> <span class="toc-nav-text">非概率模型</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B"><span class="toc-nav-number">1.2.2.</span> <span class="toc-nav-text">概率模型</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E7%AE%97%E6%B3%95"><span class="toc-nav-number">1.3.</span> <span class="toc-nav-text">算法</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-nav-number">1.4.</span> <span class="toc-nav-text">损失函数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-nav-number">1.4.1.</span> <span class="toc-nav-text">分类</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%9B%9E%E5%BD%92"><span class="toc-nav-number">1.4.2.</span> <span class="toc-nav-text">回归</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-nav-number">1.5.</span> <span class="toc-nav-text">评价指标</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ROC%E6%9B%B2%E7%BA%BF"><span class="toc-nav-number">1.5.1.</span> <span class="toc-nav-text">ROC曲线</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E4%BC%98%E5%8C%96%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-nav-number">1.6.</span> <span class="toc-nav-text">优化过拟合和欠拟合</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E9%99%8D%E4%BD%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-nav-number">1.6.1.</span> <span class="toc-nav-text">降低过拟合</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E9%99%8D%E4%BD%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-nav-number">1.6.2.</span> <span class="toc-nav-text">降低欠拟合</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">数学基础</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%AF%BC%E6%95%B0%E3%80%81%E5%81%8F%E5%AF%BC%E6%95%B0%E4%B8%8E%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">导数、偏导数与方向导数</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">机器学习中的梯度下降</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#1-1%E3%80%81%E6%89%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%AF%94%E8%BE%83"><span class="toc-nav-number">2.2.1.</span> <span class="toc-nav-text">1.1、批梯度下降比较</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#1-2%E3%80%81%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D"><span class="toc-nav-number">2.2.2.</span> <span class="toc-nav-text">1.2、最速下降</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#1-3%E3%80%81%E5%A4%9A%E5%88%86%E7%B1%BB%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-nav-number">2.2.3.</span> <span class="toc-nav-text">1.3、多分类梯度下降</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">牛顿法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Levenberg-Marquardt%E4%BF%AE%E6%AD%A3"><span class="toc-nav-number">2.3.1.</span> <span class="toc-nav-text">Levenberg-Marquardt修正</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95-vs-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-nav-number">2.3.2.</span> <span class="toc-nav-text">牛顿法 vs 梯度下降</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95-%E7%89%9B%E9%A1%BF%E6%B3%95hessian%E7%9F%A9%E9%98%B5%E7%9A%84%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3"><span class="toc-nav-number">2.3.3.</span> <span class="toc-nav-text">拟牛顿法-牛顿法hessian矩阵的优化求解</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="toc-nav-number">2.4.</span> <span class="toc-nav-text">偏差与方差</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE%E6%8E%A8%E5%AF%BC"><span class="toc-nav-number">2.4.1.</span> <span class="toc-nav-text">方差与偏差推导</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#L1%E5%92%8CL2%E6%AD%A3%E5%88%99"><span class="toc-nav-number">2.5.</span> <span class="toc-nav-text">L1和L2正则</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E8%A7%A3%E7%9A%84%E7%A8%80%E7%96%8F%E6%80%A7"><span class="toc-nav-number">2.5.1.</span> <span class="toc-nav-text">解的稀疏性</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="toc-nav-number">2.6.</span> <span class="toc-nav-text">概率论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%B8%8E%E6%A6%82%E7%8E%87"><span class="toc-nav-number">2.6.0.1.</span> <span class="toc-nav-text">极大似然与概率</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="toc-nav-number">2.6.0.2.</span> <span class="toc-nav-text">概率分布</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E4%B8%8E%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-nav-number">2.6.0.3.</span> <span class="toc-nav-text">条件概率与链式法则</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91"><span class="toc-nav-number">2.6.0.4.</span> <span class="toc-nav-text">拉普拉斯平滑</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E7%BD%AE%E4%BF%A1%E5%BA%A6%E4%B8%8E%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4"><span class="toc-nav-number">2.6.0.5.</span> <span class="toc-nav-text">置信度与置信区间</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E7%86%B5"><span class="toc-nav-number">2.7.</span> <span class="toc-nav-text">熵</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E7%9F%A9%E9%98%B5%E8%AE%BA"><span class="toc-nav-number">2.8.</span> <span class="toc-nav-text">矩阵论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%90%91%E9%87%8F%E4%B8%8E%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC"><span class="toc-nav-number">2.8.1.</span> <span class="toc-nav-text">向量与矩阵求导</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="toc-nav-number">2.8.2.</span> <span class="toc-nav-text">矩阵分解</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#2-1-%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B8%BA%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="toc-nav-number">2.8.2.1.</span> <span class="toc-nav-text">2.1 矩阵分解为特征值与特征向量</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B8%BA%E5%A5%87%E5%BC%82%E5%80%BC%E4%B8%8E%E5%A5%87%E5%BC%82%E5%90%91%E9%87%8FSVD"><span class="toc-nav-number">2.8.2.2.</span> <span class="toc-nav-text">矩阵分解为奇异值与奇异向量SVD</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%B3%95"><span class="toc-nav-number">2.9.</span> <span class="toc-nav-text">拉格朗日乘法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#KKT%E6%9D%A1%E4%BB%B6"><span class="toc-nav-number">2.9.1.</span> <span class="toc-nav-text">KKT条件</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-nav-number">2.10.</span> <span class="toc-nav-text">最小二乘法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#M-0"><span class="toc-nav-number">2.10.1.</span> <span class="toc-nav-text">M&#x3D;0</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#M-1"><span class="toc-nav-number">2.10.2.</span> <span class="toc-nav-text">M&#x3D;1</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#M-9"><span class="toc-nav-number">2.10.3.</span> <span class="toc-nav-text">M&#x3D;9</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-nav-number">2.10.4.</span> <span class="toc-nav-text">正则化</span></a></li></ol></li></ol></li></ol>
        
        </div>
      </aside>
    



      <!-- Sidebar Container -->
      <div class="
                col-lg-8 col-lg-offset-1
                col-md-10 col-md-offset-1
                sidebar-container">

        <!-- Featured Tags -->
        
        <section>
          <!-- no hr -->
          <h5>
            <a href="/tags/">特色标签</a>
          </h5>
          <div class="tags">
            
            <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
            
          </div>
        </section>
        

        <!-- Friends Blog -->
        
        <hr>
        <h5>链友</h5>
        <ul class="list-inline">

          
          <li>
            <a href="https://hexo.io/" target="_blank">Hexo</a>
          </li>
          
        </ul>
        
      </div>
    </div>
  </div>
</article>



<!-- anchorjs start -->
<!-- async load function -->
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script type="text/javascript">
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function(e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  };
</script>
<script type="text/javascript">
  //anchor-js, Doc:http://bryanbraun.github.io/anchorjs/
  async ("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js", function() {
    anchors.options = {
      visible: 'hover',
      placement: 'left',
      // icon: 'ℬ'
      icon: '❡'
    };
    anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
  });
</script>
<style>
  /* place left on bigger screen */
  @media all and (min-width: 800px) {
    .anchorjs-link {
      position: absolute;
      left: -0.75em;
      font-size: 1.1em;
      margin-top: -0.1em;
    }
  }
</style>

<!-- anchorjs end -->



		<!-- Footer (contains ThemeColor、viewer) -->
		<!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center">
          

          
            <li>
              <a target="_blank" href="https://github.com/Muzhi1920">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          

          

          

          

          

          
            <li>
              <a target="_blank" href="https://www.zhihu.com/people/awesome-yyds">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa  fa-stack-1x fa-inverse">知</i>
                </span>
              </a>
            </li>
          

          

        </ul>
        <p class="copyright text-muted">
          Copyright &copy;
          小于
          2022
          <br>
          Theme by
          <a target="_blank" rel="noopener" href="https://hexo.io/themes/">Hexo</a>
          <span style="display: inline-block; margin: 0 5px;">
            <i class="fa fa-heart"></i>
          </span>
          re-Ported by
          <a target="_blank" rel="noopener" href="https://github.com/Muzhi1920/Muzhi1920.github.io">Muzhi1920</a>
          |
          <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=Muzhi1920&repo=Muzhi1920.github.io&type=star&count=true"></iframe>
        </p>
      </div>
    </div>
  </div>
</footer>

<a id="rocket" href="#top" class=""></a>


  <!-- jQuery -->
  <script type="text/javascript" src="/js/jquery.min.js"></script>
  <!-- Bootstrap Core JavaScript -->
  <script type="text/javascript" src="/js/bootstrap.min.js"></script>
  <!-- Custom Theme JavaScript -->
  <script type="text/javascript" src="/js/hux-blog.min.js"></script>
  <!-- catalog -->
  <script async="true" type="text/javascript" src="/js/catalog.js"></script>
  <!-- totop(rocket) -->
  <script async="true" type="text/javascript" src="/js/totop.js"></script>

  
    <!-- Busuanzi JavaScript -->
    <script async="async" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

  
    <!-- Scroll start -->
    <script async="async" type="text/javascript" src="/js/scroll.js"></script>
    <!-- Scroll end -->
  

  
    <!-- LangSelect start -->
    <script type="text/javascript" src="/js/langselect.js"></script>
    <!-- LangSelect end -->
  

  
    <!-- Mouseclick -->
    <script type="text/javascript" src="/js/mouseclick.js" content='统计学习方法三要素——模型、策略、算法,高斯证明了假定误差独立同分布，在所有无偏线性估计中，最小二乘法的方差最小,SGD算法：通过误分类点优化w，b的值，使超平面向另一侧移动,朴素贝叶斯法的基本假设是条件独立性（强假设所以叫“朴素”）,决策树特征选择：信息增益、信息增益比，基尼指数（越小越好）,逻辑回归是用线性变换表示输入的对数几率模型,最大熵原理认为在所有概率模型中，熵最大的模型是最好的模型,逻辑回归与最大熵模型都属于对数线性模型，采用极大似然估计学习,逻辑回归的优化，包括改进的迭代尺度法、梯度下降法、拟牛顿法' color='#9933CC,#339933,#66CCCC,#FF99CC,#CCCCFF,#6666CC,#663399,#66CC99,#FF0033'></script>
  

  
    <!-- ribbon -->
    <script type="text/javascript" src="/js/ribbonDynamic.js"></script>
  

  






  <!-- viewer start -->
  <!-- viewer start (Picture preview) -->
  
    <script async="async" type="text/javascript" src="/js/viewer/viewer.min.js"></script>
    <script async="async" type="text/javascript" src="/js/viewer/pic-viewer.js"></script>
  

  <!-- viewer end -->


<script>
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function (e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  }

  // fastClick.js
  async ("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function () {
    var $nav = document.querySelector("nav");
    if ($nav)
      FastClick.attach($nav);
    }
  )
</script>

<!-- Because of the native support for backtick-style fenced code blocks right within the Markdown is landed in Github Pages, From V1.6, There is no need for Highlight.js, so Huxblog drops it officially. -
https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0 - https://help.github.com/articles/creating-and-highlighting-code-blocks/ -->
<!-- <script> async ("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function () { hljs.initHighlightingOnLoad(); }) </script> <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet"> -->

<!-- jquery.tagcloud.js -->
<!-- <script> // only load tagcloud.js in tag.html if ($('#tag_cloud').length !== 0) { async ("https://11010101.xyz/js/jquery.tagcloud.js", function () { $.fn.tagcloud.defaults = { // size: { start: 1, end: 1, unit: 'em' }, color: {
start: '#bbbbee', end: '#0085a1' } }; $('#tag_cloud a').tagcloud(); }) } </script> -->


		<!-- Search -->
		
		<div class="popup search-popup local-search-popup">
  <span class="popup-btn-close">
    ESC
  </span>
  <div class="container">
    <div class="row">
      <!-- <div class="col-md-9 col-md-offset-1"> -->
      <div class="col-lg-9 col-lg-offset-1 col-md-10 col-md-offset-1 local-search-content">

        <div class="local-search-header clearfix">

          <div class="local-search-input-wrapper">
            <span class="search-icon">
              <i class="fa fa-search fa-lg" style="margin: 25px 10px 25px 20px;"></i>
            </span>
            <input autocomplete="off" placeholder="搜索..." type="text" id="local-search-input">
          </div>
        </div>
        <div id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>


  
    <script src="/js/ziploader.js"></script>
  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    // monitor main search box;
    var onPopupClose = function (e) {
      $('.popup').fadeOut(300);
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $('.popup').fadeIn(300);
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }
    // get search zip version
    $.get('/searchVersion.json?t=' + (+new Date()), function (res) {
      if (localStorage.getItem('searchVersion') !== res) {
        localStorage.setItem('searchVersion', res);
        initSearchJson();
      }
    });

    function initSearchJson() {
      initLoad(['/search.flv'], {
        loadOptions: {
          success: function (obj) {
            localStorage.setItem('searchJson', obj['search.json'])
          },
          error: function (e) {
            return console.log(e)
          }
        },
        returnOptions: {
          'json': TYPE_TEXT
        },
        mimeOptions: {
          'json': 'application/json'
        }
      })
    }
    // search function;
    var searchFunc = function (search_id, content_id) {
      'use strict';
      isfetched = true;
      var datas = JSON.parse(localStorage.getItem('searchJson'));
      // console.log(search_id)
      var input = document.getElementById(search_id);
      var resultContent = document.getElementById(content_id);
      var inputEventFunction = function () {
        var searchText = input.value.trim().toLowerCase();
        var keywords = searchText.split(/[\s\-]+/);
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        var resultItems = [];
        if (searchText.length > 0) {
          // perform local searching
          datas.forEach(function (data) {
            var isMatch = false;
            var hitCount = 0;
            var searchTextCount = 0;
            var title = data.title
              ? data.title.trim()
              : '';
            var titleInLowerCase = title.toLowerCase();
            var content = data.content
              ? data.content.trim().replace(/<[^>]+>/g, "")
              : '';
            var contentInLowerCase = content.toLowerCase();
            var articleUrl = decodeURIComponent(data.url);

            var date = data.date;
            var dateTime = date.replace(/T/, " ").replace(/.000Z/, "");
            var imgUrl = data.header_img;
            


            var indexOfTitle = [];
            var indexOfContent = [];
            // only match articles with not empty titles
            keywords.forEach(function (keyword) {
              function getIndexByWord(word, text, caseSensitive) {
                var wordLen = word.length;
                if (wordLen === 0) {
                  return [];
                }
                var startPosition = 0,
                  position = [],
                  index = [];
                if (!caseSensitive) {
                  text = text.toLowerCase();
                  word = word.toLowerCase();
                }
                while ((position = text.indexOf(word, startPosition)) > -1) {
                  index.push({position: position, word: word});
                  startPosition = position + wordLen;
                }
                return index;
              }
              indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
              indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
            });
            if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
              isMatch = true;
              hitCount = indexOfTitle.length + indexOfContent.length;
            }
            // show search results
            if (isMatch) {
              // sort index by position of keyword
              [indexOfTitle, indexOfContent].forEach(function (index) {
                index.sort(function (itemLeft, itemRight) {
                  if (itemRight.position !== itemLeft.position) {
                    return itemRight.position - itemLeft.position;
                  } else {
                    return itemLeft.word.length - itemRight.word.length;
                  }
                });
              });
              // merge hits into slices
              function mergeIntoSlice(text, start, end, index) {
                var item = index[index.length - 1];
                var position = item.position;
                var word = item.word;
                var hits = [];
                var searchTextCountInSlice = 0;
                while (position + word.length <= end && index.length != 0) {
                  if (word === searchText) {
                    searchTextCountInSlice++;
                  }
                  hits.push({position: position, length: word.length});
                  var wordEnd = position + word.length;
                  // move to next position of hit
                  index.pop();
                  while (index.length != 0) {
                    item = index[index.length - 1];
                    position = item.position;
                    word = item.word;
                    if (wordEnd > position) {
                      index.pop();
                    } else {
                      break;
                    }
                  }
                }
                searchTextCount += searchTextCountInSlice;
                return {hits: hits, start: start, end: end, searchTextCount: searchTextCountInSlice};
              }
              var slicesOfTitle = [];
              if (indexOfTitle.length != 0) {
                slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
              }
              var slicesOfContent = [];
              while (indexOfContent.length != 0) {
                var item = indexOfContent[indexOfContent.length - 1];
                var position = item.position;
                var word = item.word;
                // cut out 100 characters
                var start = position - 20;
                var end = position + 80;
                if (start < 0) {
                  start = 0;
                }
                if (end < position + word.length) {
                  end = position + word.length;
                }
                if (end > content.length) {
                  end = content.length;
                }
                slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
              }
              // sort slices in content by search text's count and hits' count
              slicesOfContent.sort(function (sliceLeft, sliceRight) {
                if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                  return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                  return sliceRight.hits.length - sliceLeft.hits.length;
                } else {
                  return sliceLeft.start - sliceRight.start;
                }
              });
              // select top N slices in content
              var upperBound = parseInt('1');
              if (upperBound >= 0) {
                slicesOfContent = slicesOfContent.slice(0, upperBound);
              }
              // highlight title and content
              function highlightKeyword(text, slice) {
                var result = '';
                var prevEnd = slice.start;
                slice.hits.forEach(function (hit) {
                  result += text.substring(prevEnd, hit.position);
                  var end = hit.position + hit.length;
                  result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                  prevEnd = end;
                });
                result += text.substring(prevEnd, slice.end);
                return result;
              }
              var resultItem = '';

              // if (slicesOfTitle.length != 0) {   resultItem += "<li><a target='_blank' href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>"; } else {   resultItem += "<li><a target='_blank' href='" +
              // articleUrl + "' class='search-result-title'>" + title + "</a>"; } slicesOfContent.forEach(function (slice) {   resultItem += "<a target='_blank' href='" + articleUrl + "'><p class=\"search-result\">" + highlightKeyword(content, slice) +
              // "...</p></a>"; }); resultItem += "</li>";

              if (slicesOfTitle.length != 0) {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</div><time class='search-result-date'>" + dateTime + "</time>";
              } else {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + title + "</div><time class='search-result-date'>" + dateTime + "</time>";
              }
              slicesOfContent.forEach(function (slice) {
                resultItem += "<p class=\"search-result-content\">" + highlightKeyword(content, slice) + "...</p>";
              });
              resultItem += "</div><div class='search-result-right'><img class='media-image' src='" + imgUrl + "' width='64px' height='48px'></img></div></a>";

              resultItems.push({item: resultItem, searchTextCount: searchTextCount, hitCount: hitCount, id: resultItems.length});
            }
          })
        };

        if (keywords.length === 1 && keywords[0] === "") {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else if (resultItems.length === 0) {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else {
          resultItems.sort(function (resultLeft, resultRight) {
            if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
              return resultRight.searchTextCount - resultLeft.searchTextCount;
            } else if (resultLeft.hitCount !== resultRight.hitCount) {
              return resultRight.hitCount - resultLeft.hitCount;
            } else {
              return resultRight.id - resultLeft.id;
            }
          });
          var searchResultList = '<div class=\"search-result-list\">';
          resultItems.forEach(function (result) {
            searchResultList += result.item;
          })
          searchResultList += "</div>";
          resultContent.innerHTML = searchResultList;
        }
      }
      if ('auto' === 'auto') {
        input.addEventListener('input', inputEventFunction);
      } else {
        $('.search-icon').click(inputEventFunction);
        input.addEventListener('keypress', function (event) {
          if (event.keyCode === 13) {
            inputEventFunction();
          }
        });
      }
      // remove loading animation
      $('body').css('overflow', '');
      proceedsearch();
    }
    // handle and trigger popup window;
    $('.popup-trigger').click(function (e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc('local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });
    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function (e) {
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 && $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });

    document.addEventListener('mouseup', (e) => {
      var _con = document.querySelector(".local-search-content");
      if (_con) {
        if (!_con.contains(e.target)) {
          onPopupClose();
        }
      }
    });
  </script>


		
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
