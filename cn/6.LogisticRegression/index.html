<!DOCTYPE html>
<html lang="en">

<!-- Head tag (contains Google-Analytics、Baidu-Tongji)-->
<head>
  <!-- Google Analytics -->
  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-5GGTGE8BJS"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());

      gtag('config', 'G-5GGTGE8BJS');
    </script>
  

  <!-- Baidu Tongji -->
  

  <!-- Baidu Push -->
  

  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>

  <!-- <meta name="google-site-verification" content="lxDfCplOZbIzjhG34NuQBgu2gdyRlAtMB4utP5AgEBc"/> -->
  <meta name="google-site-verification" content="MnOJ4x6R2U5mM5X77Gw3bN3VcbjclS96MyKa6oZoMVk" />
  <meta name="baidu-site-verification" content="PpzM9WxOJU"/>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="description" content="人工智能的奇点，正在来临..."/>
  <meta name="keyword" content="AI"/>
  <link rel="shortcut icon" href="/img/avatar/tab.jpg"/>

  <!-- Place this tag in your head or just before your close body tag. -->
  <script async="async" defer="defer" src="https://buttons.github.io/buttons.js"></script>

  
    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/beantech.min.css"/>

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css"/>
    <link rel="stylesheet" href="/css/widget.css"/>
    <link rel="stylesheet" href="/css/rocket.css"/>
    <link rel="stylesheet" href="/css/signature.css"/>
    <link rel="stylesheet" href="/css/catalog.css"/>
    <link rel="stylesheet" href="/css/livemylife.css"/>

    
      <!-- wave start -->
      <link rel="stylesheet" href="/css/wave.css"/>
      <!-- wave end -->
    

    
      <!-- top start (article top hot config) -->
      <link rel="stylesheet" href="/css/top.css"/>
      <!-- top end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/css/scroll.css"/>
      <!-- ThemeColor end -->
    

    
      <!-- viewer start (Picture preview) -->
      <link rel="stylesheet" href="/css/viewer.min.css"/>
      <!-- viewer end -->
    

    
      <!-- Search start -->
      <link rel="stylesheet" href="/css/search.css"/>
      <!-- Search end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/css/themecolor.css"/>
      <!-- ThemeColor end -->
    

    

    
      <!-- gitalk start -->
      <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"> -->
      <link rel="stylesheet" href="/css/gitalk.css"/>
      <!-- gitalk end -->
    
  

  <!-- Custom Fonts -->
  <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
  <!-- Hux change font-awesome CDN to qiniu -->
  <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" type="text/css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <!-- Hux Delete, sad but pending in China <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'> <link
  href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/ css'> -->

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]-->

  <!-- ga & ba script hoook -->
  <link rel="canonical" href="https://11010101.xyz/cn/6.LogisticRegression/">
  <title>
    
      第6章 逻辑斯谛回归 - Shawn Blog
    
  </title>
<meta name="generator" content="Hexo 5.4.0"></head>


<!-- hack iOS CSS :active style -->

	<body ontouchstart="" class="body--light body--light">


		<!-- ThemeColor -->
		
		<!-- ThemeColor -->
<style type="text/css">
  .body--light {
    --light-mode: none;
    --dark-mode: block;
  }
  .body--dark {
    --light-mode: block;
    --dark-mode: none;
  }
  i.mdui-icon.material-icons.light-mode {
    display: var(--light-mode);
  }
  i.mdui-icon.material-icons.dark-mode {
    display: var(--dark-mode);
  }
</style>
<div class="toggle" onclick="document.body.classList.toggle('body--dark')">
  <i class="mdui-icon material-icons light-mode"></i>
  <i class="mdui-icon material-icons dark-mode"></i>
</div>
<script>
  //getCookieValue
  function getCookieValue(a) {
    var b = document.cookie.match('(^|[^;]+)\\s*' + a + '\\s*=\\s*([^;]+)');
    return b
      ? b.pop()
      : '';
  }
  let themeMode = 'light';
  if (getCookieValue('sb-color-mode') && (getCookieValue('sb-color-mode') !== themeMode)) {
    let dbody = document.body.classList;
    themeMode === 'dark' ? dbody.remove('body--dark') : dbody.add('body--dark');
  }

  //setCookieValue
  var toggleBtn = document.querySelector(".toggle");
  toggleBtn.addEventListener("click", function () {
    var e = document.body.classList.contains("body--dark");
    var cookieString = e
      ? "dark"
      : "light";
    var exp = new Date();
    exp.setTime(exp.getTime() + 3 * 24 * 60 * 60 * 1000); //3天过期
    document.cookie = "sb-color-mode=" + cookieString + ";expires=" + exp.toGMTString() + ";path=/";
  });
</script>

		

		<!-- Gitter -->
		
		<!-- Gitter -->
<!-- Docs:https://gitter.im/?utm_source=left-menu-logo -->
<script>
  ((window.gitter = {}).chat = {}).options = {
    room: 'touch_fish/lie_down'
  };
</script>
<script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>

		

		<!-- Navigation (contains search)-->
		<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Shawn Blog</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <!-- Known Issue, found by Hux: <nav>'s height woule be hold on by its content. so, when navbar scale out, the <nav> will cover tags. also mask any touch event of tags, unfortunately. -->
    <div id="huxblog_navbar">
      <div class="navbar-collapse">
        <ul class="nav navbar-nav navbar-right">
          <li>
            <a href="/">主页</a>
          </li>

          
          
          
          
          <li>
            <a href="/about/">
              
              关于我
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/archive/">
              
              归档
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/categories/">
              
              分类
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/tags/">
              
              标签
              
              
            </a>
          </li>
          
          

          
          <li>
            <a class="popup-trigger">
              <span class="search-icon"></span>搜索</a>
          </li>
          

          <!-- LangSelect -->
          
          
          
          
          
        </ul>
      </div>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container -->
</nav>
<!-- progress -->
<div id="progress">
  <div class="line" style="width: 0%;"></div>
</div>

<script>
  // Drop Bootstarp low-performance Navbar Use customize navbar with high-quality material design animation in high-perf jank-free CSS3 implementation
  var $body = document.body;
  var $toggle = document.querySelector('.navbar-toggle');
  var $navbar = document.querySelector('#huxblog_navbar');
  var $collapse = document.querySelector('.navbar-collapse');

  $toggle.addEventListener('click', handleMagic)

  function handleMagic(e) {
    if ($navbar.className.indexOf('in') > 0) {
      // CLOSE
      $navbar.className = " ";
      // wait until animation end.
      setTimeout(function() {
        // prevent frequently toggle
        if ($navbar.className.indexOf('in') < 0) {
          $collapse.style.height = "0px"
        }
      }, 400)
    } else {
      // OPEN
      $collapse.style.height = "auto"
      $navbar.className += " in";
    }
  }
</script>


		<!-- Post Header (contains intro-header、signature、wordcount、busuanzi、waveoverlay) -->
		<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->

  <style type="text/css">
    .body--light {
      /* intro-header */
      --intro-header-background-image-url-home: url('/img/header_img/new_home_bg.jpg');
      --intro-header-background-image-url-post: url('');
      --intro-header-background-image-url-page: url('/img/header_img/archive_bg.jpg');
    }
    .body--dark {
      --intro-header-background-image-url-home: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/img/header_img/new_home_bg.jpg');
      --intro-header-background-image-url-post: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('');
      --intro-header-background-image-url-page: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/img/header_img/archive_bg.jpg');
    }

    header.intro-header {
       /*post*/
        background-image: var(--intro-header-background-image-url-post);
        /* background-image: url(''); */
      
    }

    
      #signature {/*signature*/
        background-image: url('/img/signature/my_signature.png');
      }
    
  </style>





<header class="intro-header">
  <!-- Signature -->
  <div id="signature">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
          
          <div class="post-heading">
            <div class="tags">
              
              <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
              
            </div>
            <h1>第6章 逻辑斯谛回归</h1>
            <h2 class="subheading">logistic regression</h2>
            <span class="meta">
              Posted by 小于 on
              2022-03-21
            </span>


            
            <!-- WordCount start -->
            <div class="blank_box"></div>
            <span class="meta">
              Estimated Reading Time <span class="post-count">19</span> Minutes
            </span>
            <div class="blank_box"></div>
            <span class="meta">
              Words <span class="post-count">4.4k</span> In Total
            </span>
            <div class="blank_box"></div>
            <!-- WordCount end -->
            
            
            <!-- 不蒜子统计 start -->
            <span class="meta" id="busuanzi_container_page_pv">
              Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
            </span>
            <!-- 不蒜子统计 end -->
            


          </div>
          
        </div>
      </div>
    </div>
  </div>

  
  <!-- waveoverlay start -->
  <div class="preview-overlay">
    <svg class="preview-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
      <defs>
        <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path>
      </defs>
      <g class="preview-parallax">
        <use xlink:href="#gentle-wave" x="48" y="0" fill=var(--gentle-wave1)></use>
        <use xlink:href="#gentle-wave" x="48" y="3" fill=var(--gentle-wave2)></use>
        <use xlink:href="#gentle-wave" x="48" y="5" fill=var(--gentle-wave3)></use>
        <use xlink:href="#gentle-wave" x="48" y="7" fill=var(--gentle-wave)></use>
      </g>
    </svg>
  </div>
  <!-- waveoverlay end -->
  

</header>



		<!-- Main Content (Post contains
	Pager、
	tip、
	socialshare、
	gitalk、gitment、disqus-comment、
	Catalog、
	Sidebar、
	Featured-Tags、
	Friends Blog、
	anchorjs、
	) -->
		<!-- Modify by Yu-Hsuan Yen -->
<!-- Post Content -->
<article>
  <div class="container">
    <div class="row">
      <!-- Post Container -->
      <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1 post-container">

        <h1 id="第6章-逻辑斯谛回归"><a href="#第6章-逻辑斯谛回归" class="headerlink" title="第6章 逻辑斯谛回归"></a>第6章 逻辑斯谛回归</h1><blockquote>
<p>附详细梯度下降优化算法</p>
</blockquote>
<p>1．逻辑斯谛回归模型是由以下条件概率分布表示的分类模型。</p>
<script type="math/tex; mode=display">P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1</script><script type="math/tex; mode=display">P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}</script><p>逻辑斯谛回归模型是由输入的线性函数表示的输出的对数几率模型。</p>
<p>2．最大熵模型是由以下条件概率分布表示的分类模型。</p>
<script type="math/tex; mode=display">P_{w}(y | x)=\frac{1}{Z_{w}(x)} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)</script><script type="math/tex; mode=display">Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)</script><p>其中，$Z_w(x)$是规范化因子，$f_i$为特征函数，$w_i$为特征的权值。</p>
<p>3．最大熵模型可以由最大熵原理推导得出。最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。最大熵原理应用到分类模型的学习中，有以下约束最优化问题：</p>
<script type="math/tex; mode=display">\min -H(P)=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)</script><script type="math/tex; mode=display">s.t.  \quad P\left(f_{i}\right)-\tilde{P}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n</script><script type="math/tex; mode=display">\sum_{y} P(y | x)=1</script><p>求解此最优化问题的对偶问题得到最大熵模型。</p>
<p>4．逻辑斯谛回归模型与最大熵模型都属于对数线性模型。</p>
<p>5．逻辑斯谛回归模型及最大熵模型学习一般采用极大似然估计，或正则化的极大似然估计。逻辑斯谛回归模型及最大熵模型学习可以形式化为无约束最优化问题。求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法。</p>
<p>回归模型：$f(x) = \frac{1}{1+e^{-wx}}$</p>
<p>其中wx线性函数：$wx =w_0\cdot x_0 + w_1\cdot x_1 + w_2\cdot x_2 +…+w_n\cdot x_n,(x_0=1)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> exp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line">    df = pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">    df[<span class="string">&#x27;label&#x27;</span>] = iris.target</span><br><span class="line">    df.columns = [<span class="string">&#x27;sepal length&#x27;</span>, <span class="string">&#x27;sepal width&#x27;</span>, <span class="string">&#x27;petal length&#x27;</span>, <span class="string">&#x27;petal width&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    data = np.array(df.iloc[:<span class="number">100</span>, [<span class="number">0</span>,<span class="number">1</span>,-<span class="number">1</span>]])</span><br><span class="line">    <span class="comment"># print(data)</span></span><br><span class="line">    <span class="keyword">return</span> data[:,:<span class="number">2</span>], data[:,-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = create_data()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="scikit-learn实例"><a href="#scikit-learn实例" class="headerlink" title="scikit-learn实例"></a>scikit-learn实例</h3><h4 id="sklearn-linear-model-LogisticRegression"><a href="#sklearn-linear-model-LogisticRegression" class="headerlink" title="sklearn.linear_model.LogisticRegression"></a>sklearn.linear_model.LogisticRegression</h4><p>solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：</p>
<ul>
<li>a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。</li>
<li>b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li>
<li>c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li>
<li>d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf = LogisticRegression(max_iter=<span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression(max_iter=200)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>
<pre><code>1.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(clf.coef_, clf.intercept_)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 2.71486791 -2.5494056 ]] [-6.99789659]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x_ponits = np.arange(<span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">y_ = -(clf.coef_[<span class="number">0</span>][<span class="number">0</span>]*x_ponits + clf.intercept_)/clf.coef_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">plt.plot(x_ponits, y_)</span><br><span class="line"></span><br><span class="line">plt.plot(X[:<span class="number">50</span>, <span class="number">0</span>], X[:<span class="number">50</span>, <span class="number">1</span>], <span class="string">&#x27;bo&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">plt.plot(X[<span class="number">50</span>:, <span class="number">0</span>], X[<span class="number">50</span>:, <span class="number">1</span>], <span class="string">&#x27;bo&#x27;</span>, color=<span class="string">&#x27;orange&#x27;</span>, label=<span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;sepal length&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;sepal width&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x7f399c46d550&gt;
</code></pre><p><img src="/cn/6.LogisticRegression/output_12_1.png" alt="png"></p>
<h2 id="第6章Logistic回归与最大熵模型-习题"><a href="#第6章Logistic回归与最大熵模型-习题" class="headerlink" title="第6章Logistic回归与最大熵模型-习题"></a>第6章Logistic回归与最大熵模型-习题</h2><p><strong>以下需要背诵</strong></p>
<p>Logistic分布属于指数分布族，是广义线性可分模型。参考：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/40Z9eDn-oVDr_UrdT6_cTQ">https://mp.weixin.qq.com/s/40Z9eDn-oVDr_UrdT6_cTQ</a></p>
<ol>
<li>假设数据线性可分，可使用广义线性模型</li>
<li>GLM要求概率分布服从指数分布族</li>
<li>二分类数据服从伯努利分布，属于指数分布族</li>
<li>通过e指数变换为指数形式，得到预估概率$\hat{y}$的定义，就是Logistic函数</li>
</ol>
<p>通过，对应相等得到伯努利分布属于指数分布族。</p>
<h3 id="习题6-1"><a href="#习题6-1" class="headerlink" title="习题6.1"></a>习题6.1</h3><p>&emsp;&emsp;确认Logistic分布属于指数分布族。</p>
<p><strong>解答：</strong><br><strong>第1步：</strong><br>首先给出指数分布族的定义：<br>对于随机变量$x$，在给定参数$\eta$下，其概率分别满足如下形式：<script type="math/tex">p(x|\eta)=h(x)g(\eta)\exp(\eta^Tu(x))</script>我们称之为<strong>指数分布族</strong>。<br>其中：<br>$x$：可以是标量或者向量，可以是离散值也可以是连续值<br>$\eta$：自然参数<br>$g(\eta)$：归一化系数<br>$h(x),u(x)$：$x$的某个函数</p>
<hr>
<p><strong>第2步：</strong>证明伯努利分布属于指数分布族<br>伯努利分布：$\varphi$是$y=1$的概率，即$P(Y=1)=\varphi$<br>$\begin{aligned}<br>P(y|\varphi)<br>&amp;= \varphi^y (1-\varphi)^{(1-y)} \\<br>&amp;= (1-\varphi) \varphi^y (1-\varphi)^{(-y)} \\<br>&amp;= (1-\varphi) (\frac{\varphi}{1-\varphi})^y \\<br>&amp;= (1-\varphi) \exp\left(y \ln \frac{\varphi}{1-\varphi} \right) \\<br>&amp;= \frac{1}{1+e^\eta} \exp (\eta y)<br>\end{aligned}$</p>
<p>其中，$\displaystyle \eta=\ln \frac{\varphi}{1-\varphi} \Leftrightarrow \varphi = \frac{1}{1+e^{-\eta}}$<br>将$y$替换成$x$，可得$\displaystyle P(x|\eta) = \frac{1}{1+e^\eta} \exp (\eta x)$<br>对比可知，伯努利分布属于指数分布族，其中$\displaystyle h(x) = 1, g(\eta)= \frac{1}{1+e^\eta}, u(x)=x$</p>
<hr>
<p><strong>第3步：</strong><br>广义线性模型（GLM）必须满足三个假设：</p>
<ol>
<li>$y | x;\theta \sim ExponentialFamily(\eta)$，即假设预测变量$y$在给定$x$，以$\theta$为参数的条件概率下，属于以$\eta$作为自然参数的指数分布族；</li>
<li>给定$x$，求解出以$x$为条件的$T(y)$的期望$E[T(y)|x]$，即算法输出为$h(x)=E[T(y)|x]$</li>
<li>满足$\eta=\theta^T x$，即自然参数和输入特征向量$x$之间线性相关，关系由$\theta$决定，仅当$\eta$是实数时才有意义，若$\eta$是一个向量，则$\eta_i=\theta_i^T x$</li>
</ol>
<hr>
<p><strong>第4步：</strong>推导伯努利分布的GLM<br>已知伯努利分布属于指数分布族，对给定的$x,\eta$，求解期望：<script type="math/tex">\begin{aligned}
h_{\theta}(x)
&= E[y|x;\theta] \\
&= 1 \cdot p(y=1)+ 0 \cdot p(y=0) \\
&= \varphi \\
&= \frac{1}{1+e^{-\eta}} \\
&= \frac{1}{1+e^{-\theta^T x}}
\end{aligned}</script>可得到Logistic回归算法，故Logistic分布属于指数分布族，得证。</p>
<h3 id="习题6-2"><a href="#习题6-2" class="headerlink" title="习题6.2"></a>习题6.2</h3><p>&emsp;&emsp;写出Logistic回归模型学习的梯度下降算法。</p>
<p><strong>解答：</strong><br>对于Logistic模型：</p>
<script type="math/tex; mode=display">P(Y=1 | x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)} \\ P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x+b)}</script><p>对数似然函数为：$\displaystyle L(w)=\sum_{i=1}^N \left[y_i (w \cdot x_i)-\log \left(1+\exp (w \cdot x_i)\right)\right]$<br>似然函数求偏导，可得$\displaystyle \frac{\partial L(w)}{\partial w^{(j)}}=\sum_{i=1}^N\left[x_i^{(j)} \cdot y_i-\frac{\exp (w \cdot x_i) \cdot x_i^{(j)}}{1+\exp (w \cdot x_i)}\right]$<br>梯度函数为：$\displaystyle \nabla L(w)=\left[\frac{\partial L(w)}{\partial w^{(0)}}, \cdots, \frac{\partial L(w)}{\partial w^{(m)}}\right]$<br>Logistic回归模型学习的梯度下降算法：<br>(1) 取初始值$x^{(0)} \in R$，置$k=0$<br>(2) 计算$f(x^{(k)})$<br>(3) 计算梯度$g_k=g(x^{(k)})$，当$ | g_k | &lt; \varepsilon $时，停止迭代，令$x^<em> = x^{(k)}$；否则，求$\lambda_k$，使得$\displaystyle f(x^{(k)}+\lambda_k g_k) = \max_{\lambda \geqslant 0}f(x^{(k)}+\lambda g_k)$<br>(4) 置$x^{(k+1)}=x^{(k)}+\lambda_k g_k$，计算$f(x^{(k+1)})$，当$|f(x^{(k+1)}) - f(x^{(k)})| &lt; \varepsilon$或 $|x^{(k+1)} - x^{(k)}| &lt; \varepsilon$时，停止迭代，令$x^</em> = x^{(k+1)}$<br>(5) 否则，置$k=k+1$，转(3)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像显示中文</span></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;Microsoft YaHei&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, learn_rate=<span class="number">0.1</span>, max_iter=<span class="number">10000</span>, tol=<span class="number">1e-2</span></span>):</span></span><br><span class="line">        self.learn_rate = learn_rate  <span class="comment"># 学习率</span></span><br><span class="line">        self.max_iter = max_iter  <span class="comment"># 迭代次数</span></span><br><span class="line">        self.tol = tol  <span class="comment"># 迭代停止阈值</span></span><br><span class="line">        self.w = <span class="literal">None</span>  <span class="comment"># 权重</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;将原始X末尾加上一列，该列数值全部为1&quot;&quot;&quot;</span></span><br><span class="line">        row = X.shape[<span class="number">0</span>]</span><br><span class="line">        y = np.ones(row).reshape(row, <span class="number">1</span>)</span><br><span class="line">        X_prepro = np.hstack((X, y))</span><br><span class="line">        <span class="keyword">return</span> X_prepro</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmod</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X_train, y_train</span>):</span></span><br><span class="line">        X = self.preprocessing(X_train)</span><br><span class="line">        y = y_train.T</span><br><span class="line">        <span class="comment"># 初始化权重w</span></span><br><span class="line">        self.w = np.array([[<span class="number">0</span>] * X.shape[<span class="number">1</span>]], dtype=np.<span class="built_in">float</span>)</span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> loop <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            <span class="comment"># 计算梯度</span></span><br><span class="line">            z = np.dot(X, self.w.T)</span><br><span class="line">            grad = X * (y - self.sigmod(z))</span><br><span class="line">            grad = grad.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 利用梯度的绝对值作为迭代中止的条件</span></span><br><span class="line">            <span class="keyword">if</span> (np.<span class="built_in">abs</span>(grad) &lt;= self.tol).<span class="built_in">all</span>():</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 更新权重w 梯度上升——求极大值</span></span><br><span class="line">                self.w += self.learn_rate * grad</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;迭代次数：&#123;&#125;次&quot;</span>.<span class="built_in">format</span>(k))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;最终梯度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(grad))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;最终权重：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.w[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        p = self.sigmod(np.dot(self.preprocessing(x), self.w.T))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Y=1的概率被估计为：&#123;:.2%&#125;&quot;</span>.<span class="built_in">format</span>(p[<span class="number">0</span>][<span class="number">0</span>]))  <span class="comment"># 调用score时，注释掉</span></span><br><span class="line">        p[np.where(p &gt; <span class="number">0.5</span>)] = <span class="number">1</span></span><br><span class="line">        p[np.where(p &lt; <span class="number">0.5</span>)] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        y_c = self.predict(X)</span><br><span class="line">        error_rate = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(y_c - y.T)) / y_c.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - error_rate</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        <span class="comment"># 分离正负实例点</span></span><br><span class="line">        y = y[<span class="number">0</span>]</span><br><span class="line">        X_po = X[np.where(y == <span class="number">1</span>)]</span><br><span class="line">        X_ne = X[np.where(y == <span class="number">0</span>)]</span><br><span class="line">        <span class="comment"># 绘制数据集散点图</span></span><br><span class="line">        ax = plt.axes(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">        x_1 = X_po[<span class="number">0</span>, :]</span><br><span class="line">        y_1 = X_po[<span class="number">1</span>, :]</span><br><span class="line">        z_1 = X_po[<span class="number">2</span>, :]</span><br><span class="line">        x_2 = X_ne[<span class="number">0</span>, :]</span><br><span class="line">        y_2 = X_ne[<span class="number">1</span>, :]</span><br><span class="line">        z_2 = X_ne[<span class="number">2</span>, :]</span><br><span class="line">        ax.scatter(x_1, y_1, z_1, c=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;正实例&quot;</span>)</span><br><span class="line">        ax.scatter(x_2, y_2, z_2, c=<span class="string">&quot;b&quot;</span>, label=<span class="string">&quot;负实例&quot;</span>)</span><br><span class="line">        ax.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">        <span class="comment"># 绘制p=0.5的区分平面</span></span><br><span class="line">        x = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        y = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        x_3, y_3 = np.meshgrid(x, y)</span><br><span class="line">        a, b, c, d = self.w[<span class="number">0</span>]</span><br><span class="line">        z_3 = -(a * x_3 + b * y_3 + d) / c</span><br><span class="line">        ax.plot_surface(x_3, y_3, z_3, alpha=<span class="number">0.5</span>)  <span class="comment"># 调节透明度</span></span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">X_train = np.array([[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                    [<span class="number">2</span>, -<span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">y_train = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 构建实例，进行训练</span></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.draw(X_train, y_train)</span><br></pre></td></tr></table></figure>
<pre><code>迭代次数：3232次
最终梯度：[ 0.00144779  0.00046133  0.00490279 -0.00999848]
最终权重：[  2.96908597   1.60115396   5.04477438 -13.43744079]
</code></pre><p><img src="/cn/6.LogisticRegression/output_18_2.png" alt="png"></p>
<h3 id="习题6-3"><a href="#习题6-3" class="headerlink" title="习题6.3"></a>习题6.3</h3><p>&emsp;&emsp;写出最大熵模型学习的DFP算法。（关于一般的DFP算法参见附录B）</p>
<p><strong>解答：</strong><br><strong>第1步：</strong><br>最大熵模型为：<script type="math/tex">\begin{array}{cl}
{\max } & {H(p)=-\sum_{x, y} P(x) P(y | x) \log P(y | x)} \\
{\text {st.}} &
{E_p(f_i)-E_{\hat{p}}(f_i)=0, \quad i=1,2, \cdots,n} \\
& {\sum_y P(y | x)=1}
\end{array}</script></p>
<p>引入拉格朗日乘子，定义拉格朗日函数：</p>
<script type="math/tex; mode=display">
L(P, w)=\sum_{xy} P(x) P(y | x) \log P(y | x)+w_0 \left(1-\sum_y P(y | x)\right) \\
+\sum_{i=1} w_i\left(\sum_{xy} P(x, y) f_i(x, y)-\sum_{xy} P(x, y) P(y | x) f_i(x, y)\right)</script><p>最优化原始问题为：</p>
<script type="math/tex; mode=display">\min_{P \in C} \max_{w} L(P,w)</script><p>对偶问题为：</p>
<script type="math/tex; mode=display">\max_{w} \min_{P \in C} L(P,w)</script><p>令</p>
<script type="math/tex; mode=display">\Psi(w) = \min_{P \in C} L(P,w) = L(P_w, w)</script><p>$\Psi(w)$称为对偶函数，同时，其解记作</p>
<script type="math/tex; mode=display">P_w = \mathop{\arg \min}_{P \in C} L(P,w) = P_w(y|x)</script><p>求$L(P,w)$对$P(y|x)$的偏导数，并令偏导数等于0，解得：</p>
<script type="math/tex; mode=display">
P_w(y | x)=\frac{1}{Z_w(x)} \exp \left(\sum_{i=1}^n w_i f_i (x, y)\right)</script><p>其中：</p>
<script type="math/tex; mode=display">Z_w(x)=\sum_y \exp \left(\sum_{i=1}^n w_i f_i(x, y)\right)</script><p>则最大熵模型目标函数表示为</p>
<script type="math/tex; mode=display">\varphi(w)=\min_{w \in R_n} \Psi(w) = \sum_{x} P(x) \log \sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)-\sum_{x, y} P(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)</script><p><strong>第2步：</strong><br>DFP的$G_{k+1}$的迭代公式为：</p>
<script type="math/tex; mode=display">G_{k+1}=G_k+\frac{\delta_k \delta_k^T}{\delta_k^T y_k}-\frac{G_k y_k y_k^T G_k}{y_k^T G_k y_k}</script><p><strong>最大熵模型的DFP算法：</strong><br>输入：目标函数$\varphi(w)$，梯度$g(w) = \nabla g(w)$，精度要求$\varepsilon$；<br>输出：$\varphi(w)$的极小值点$w^<em>$<br>(1)选定初始点$w^{(0)}$，取$G_0$为正定对称矩阵，置$k=0$<br>(2)计算$g_k=g(w^{(k)})$，若$|g_k| &lt; \varepsilon$，则停止计算，得近似解$w^</em>=w^{(k)}$，否则转(3)<br>(3)置$p_k=-G_kg_k$<br>(4)一维搜索：求$\lambda_k$使得<script type="math/tex">\varphi\left(w^{(k)}+\lambda_k P_k\right)=\min _{\lambda \geqslant 0} \varphi\left(w^{(k)}+\lambda P_{k}\right)</script>(5)置$w^{(k+1)}=w^{(k)}+\lambda_k p_k$<br>(6)计算$g_{k+1}=g(w^{(k+1)})$，若$|g_{k+1}| &lt; \varepsilon$，则停止计算，得近似解$w^*=w^{(k+1)}$；否则，按照迭代式算出$G_{k+1}$<br>(7)置$k=k+1$，转(3)</p>
<hr>
<p>参考代码：<a target="_blank" rel="noopener" href="https://github.com/wzyonggege/statistical-learning-method">https://github.com/wzyonggege/statistical-learning-method</a></p>
<p>本文代码更新地址：<a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">https://github.com/fengdu78/lihang-code</a></p>
<p>习题解答：<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/statistical-learning-method-solutions-manual">https://github.com/datawhalechina/statistical-learning-method-solutions-manual</a></p>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降是一种<strong>优化算法</strong>，通过<strong>迭代</strong>的方式寻找模型的<strong>最优参数</strong>；当目标函数是<strong>凸函数</strong>时，梯度下降的解是全局最优解；但在一般情况下，<strong>梯度下降无法保证全局最优</strong>。<strong>负梯度</strong>中的每一项可以认为传达了<strong>两个信息</strong>:</p>
<ul>
<li>正负号在告诉输入向量应该调大还是调小（正调大，负调小）；</li>
<li>每一项的相对大小表明每个参数对函数值达到最值的<strong>影响程度</strong></li>
</ul>
<h3 id="批梯度下降（Batch-SGD）-VS-随机梯度下降"><a href="#批梯度下降（Batch-SGD）-VS-随机梯度下降" class="headerlink" title="批梯度下降（Batch SGD） VS  随机梯度下降"></a>批梯度下降（Batch SGD） VS  随机梯度下降</h3><ul>
<li><strong>批梯度下降</strong>在每次对模型参数进行更新时，需要遍历所有数据，得到平均损失<br><img src="/cn/6.LogisticRegression/e1.png" alt></li>
<li><strong>随机梯度下降</strong>每次使用单个样本的损失来近似平均损失<br><img src="/cn/6.LogisticRegression/e2.png" alt><h3 id="小批量随机梯度下降（mini-batch-SGD）"><a href="#小批量随机梯度下降（mini-batch-SGD）" class="headerlink" title="小批量随机梯度下降（mini batch SGD）"></a>小批量随机梯度下降（mini batch SGD）</h3></li>
<li>为了降低随机梯度的<strong>方差</strong>，使模型迭代更加稳定，实践中会使用<strong>一批</strong>随机数据的损失来近似平均损失。</li>
<li>使用批训练的另一个主要目的，是为了利用高度优化的<strong>矩阵运算</strong>以及<strong>并行计算框架</strong>。</li>
</ul>
<h3 id="“批”的大小对优化效果的影响"><a href="#“批”的大小对优化效果的影响" class="headerlink" title="“批”的大小对优化效果的影响"></a>“批”的大小对优化效果的影响</h3><ul>
<li>批越小：则泛化误差更好，学习更加充分，引入噪声正则化效果，学习率小保证稳定性故收敛慢；</li>
<li>批越大则梯度估计更加精确，训练加速</li>
</ul>
<blockquote>
<blockquote>
<p>当批的大小为 <strong>2 的幂</strong>时能充分利用矩阵运算操作，所以批的大小一般取 32、64、128、256 等。</p>
</blockquote>
</blockquote>
<h3 id="随机梯度下降存在的问题"><a href="#随机梯度下降存在的问题" class="headerlink" title="随机梯度下降存在的问题"></a>随机梯度下降存在的问题</h3><ul>
<li>梯度下降陷入<strong>局部极值点</strong>，以及遇到“<strong>峡谷</strong>”和“<strong>鞍点</strong>”两种情况</li>
<li><strong>峡谷</strong>：准确的梯度方向应该沿着坡的方向向下，但粗糙的梯度估计导致在两个峭壁间来回震荡。</li>
<li><strong>鞍点</strong>：落入鞍点导致优化很可能就会停滞下来。</li>
</ul>
<p><img src="/cn/6.LogisticRegression/an.png" alt="avatar"></p>
<h2 id="改进的随机梯度下降"><a href="#改进的随机梯度下降" class="headerlink" title="改进的随机梯度下降"></a>改进的随机梯度下降</h2><p>SGD 的改进遵循两个方向：<strong>惯性保持</strong>和<strong>环境感知</strong></p>
<ul>
<li><strong>惯性保持</strong>指的是加入动量SGD 算法；针对鞍点</li>
<li><strong>环境感知</strong>指的是<strong>自适应</strong>地确定<strong>每个参数的学习速率</strong>；针对峡谷</li>
</ul>
<h3 id="动量算法"><a href="#动量算法" class="headerlink" title="动量算法"></a>动量算法</h3><h4 id="Momentum动量"><a href="#Momentum动量" class="headerlink" title="Momentum动量"></a>Momentum动量</h4><p>引入<strong>动量</strong>方法一方面是为了解决“峡谷”和“鞍点”问题；一方面也可以用于SGD 加速，特别是针对<strong>高曲率</strong>、小幅但是方向一致的梯度。在鞍点处因为<strong>惯性</strong>的作用，更有可能离开平地。<br><img src="/cn/6.LogisticRegression/line.png" alt="avatar"></p>
<blockquote>
<blockquote>
<p>将过去时间steps的梯度矢量累加到当前去更新梯度，致使梯度相同的方向的维度，动量项增加，加快SGD的正确方向，并抑制震荡。</p>
<script type="math/tex; mode=display">v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta) \\
\theta = \theta-v_t</script><p>缺点：梯度太大，略过最小值，在最小值附近震荡。</p>
<h4 id="NAG-动量"><a href="#NAG-动量" class="headerlink" title="NAG 动量"></a>NAG 动量</h4><script type="math/tex; mode=display">v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta - \gamma v_{t-1}) \\
\theta = \theta-v_t</script><p><img src="/cn/6.LogisticRegression/mn.png" alt><br>具有前瞻性，前瞻位置估计梯度</p>
</blockquote>
</blockquote>
<h4 id="Momentum和NAG动量的区别"><a href="#Momentum和NAG动量的区别" class="headerlink" title="Momentum和NAG动量的区别"></a>Momentum和NAG动量的区别</h4><ul>
<li>momentum：计算梯度-&gt;加上动量加速$\gamma * v$-&gt;更新参数；</li>
<li>nesterov：加上动量加速$\gamma <em> v$去计算梯度-&gt;加上动量加速$\gamma </em> v$-&gt;更新参数。</li>
</ul>
<h3 id="自适应学习率的优化算法"><a href="#自适应学习率的优化算法" class="headerlink" title="自适应学习率的优化算法"></a>自适应学习率的优化算法</h3><h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><ul>
<li>该算法的思想是独立地适应模型的每个参数：具有较大偏导的参数相应有一个较大的学习率，而具有小偏导的参数则对应一个较小的学习率。学习率会反比于其<strong>历史梯度平方值总和的平方根</strong></li>
</ul>
<h5 id="AdaGrad-算法描述"><a href="#AdaGrad-算法描述" class="headerlink" title="AdaGrad 算法描述"></a>AdaGrad 算法描述</h5><script type="math/tex; mode=display">r=r+g*g</script><script type="math/tex; mode=display">\Delta\theta=-\frac{\eta}{\sqrt{r+\delta}}*g</script><p>注意：全局学习率$\eta$并没有更新，而是每次应用时被缩放</p>
<h5 id="AdaGrad-存在的问题"><a href="#AdaGrad-存在的问题" class="headerlink" title="AdaGrad 存在的问题"></a>AdaGrad 存在的问题</h5><p>学习率是一直递减的，训练后期学习率过小会导致训练困难，甚至提前结束。</p>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>AdaGrad 根据平方梯度的<strong>整个历史</strong>来收缩学习率，可能使得学习率在达到局部最小值之前就变得太小而难以继续训练;RMSProp 主要是为了解决 AdaGrad 方法中<strong>学习率过度衰减</strong>的问题</p>
<hr>
<p>RMSProp 使用<strong>指数衰减平均</strong>（递归定义）以丢弃遥远的历史，使其能够在找到某个“凸”结构后快速收敛；此外，RMSProp 还加入了一个超参数 <code>ρ</code> 用于控制衰减速率。</p>
<h5 id="RMSProp-算法描述"><a href="#RMSProp-算法描述" class="headerlink" title="RMSProp 算法描述"></a>RMSProp 算法描述</h5><script type="math/tex; mode=display">r=\rho r+ (1-\rho)g*g</script><script type="math/tex; mode=display">\Delta\theta=-\frac{\eta}{\sqrt{r+\delta}}*g</script><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><ul>
<li>加入<strong>历史梯度平方的指数衰减平均</strong>（<code>r</code>）</li>
<li>保留了<strong>历史梯度的指数衰减平均</strong>（<code>s</code>），相当于<strong>动量</strong>。<br>Adam=Momentum（一阶）+RMSProp（二阶）<h5 id="Adam-算法描述"><a href="#Adam-算法描述" class="headerlink" title="Adam 算法描述"></a>Adam 算法描述</h5><img src="/cn/6.LogisticRegression/adam.png" alt="avatar"><script type="math/tex; mode=display">m_t=\gamma m_{t-1}+(1-\gamma)g_t\\
v_t=\rho v_{t-1}+(1-\rho)g_t^2\\
\hat{m_t}=\frac{m_t}{1-\gamma}\\
\hat{n_t}=\frac{n_t}{1-\rho}\\
\theta:=\theta-\frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}\hat{m_t}</script></li>
</ul>
<h2 id="如何选择这些优化算法"><a href="#如何选择这些优化算法" class="headerlink" title="如何选择这些优化算法"></a>如何选择这些优化算法</h2><ul>
<li>各自适应学习率的优化算法表现不分伯仲，没有哪个算法能在所有任务上脱颖而出；</li>
<li>目前，最流行并且使用很高的优化算法包括 SGD、带动量的 SGD、RMSProp、带动量的 RMSProp、AdaDelta 和 Adam。</li>
</ul>
<h3 id="各优化算法的可视化"><a href="#各优化算法的可视化" class="headerlink" title="各优化算法的可视化"></a>各优化算法的可视化</h3><ul>
<li><p>SGD 各优化方法在损失曲面上的表现<br><img src="/cn/6.LogisticRegression/com.gif" alt="avatar"></p>
</li>
<li><p>SGD 各优化方法在<strong>鞍点</strong>处上的表现<br><img src="/cn/6.LogisticRegression/comp.gif" alt="avatar"></p>
</li>
</ul>


        <hr>
        <!-- Pager -->
        <ul class="pager">
          
          <li class="previous">
            <a href="/cn/7.support-vector-machine/" data-toggle="tooltip" data-placement="top" title="第7章 支持向量机">&larr; Previous Post</a>
          </li>
          
          
          <li class="next">
            <a href="/cn/5.DecisonTree/" data-toggle="tooltip" data-placement="top" title="第5章 决策树">Next Post &rarr;</a>
          </li>
          
        </ul>

        
        <!-- tip start -->
        <!-- tip -->
<!-- tip start -->
<div class="tip">
  <p>
    
      如果您喜欢此博客或发现它对您有用，则欢迎对此发表评论。 也欢迎您共享此博客，以便更多人可以参与。 如果博客中使用的图像侵犯了您的版权，请与作者联系以将其删除。 谢谢 ！
    
  </p>
</div>
<!-- tip end -->

        <!-- tip end -->
        

        
        <!-- Sharing Srtart -->
        <!-- Social Social Share Post -->
<!-- Docs:https://github.com/overtrue/share.js -->

<div class="social-share" data-initialized="true" data-disabled="tencent ,douban ,qzone ,linkedin ,facebook ,google ,diandian" data-wechat-qrcode-helper="" align="center">
  <ul class="list-inline text-center social-share-ul">
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-twitter">
        <i class="fa fa-twitter fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a class="social-share-icon icon-wechat">
        <i class="fa fa-weixin fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-weibo">
        <i class="fa fa-weibo fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-qq">
        <i class="fa fa-qq fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon" href="mailto:?subject=第6章 逻辑斯谛回归&body=Hi,I found this website and thought you might like it https://11010101.xyz/cn/6.LogisticRegression/">
        <i class="fa fa-envelope fa-1x" aria-hidden="true"></i>
      </a>
    </li>
  </ul>
</div>

<!-- css & js -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css"> -->
<script defer="defer" async="true" src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>

        <!-- Sharing End -->
        
        <hr>

        <!-- comments start -->
        <!-- 1. gitalk comment -->

  <!-- gitalk start -->
  <!-- Docs:https://github.com/gitalk/gitalk/blob/master/readme-cn.md -->

  <div id="gitalk-container"></div>

  
    <!-- <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.js"></script> -->
    <script src="/js/comment/gitalk.js"></script>
  

  <script>
    var gitalk = new Gitalk({
      clientID: 'a9a645b881c78d12baa8',
      clientSecret: '144741a3fa325fe22d7207d5698374724cd412b7',
      repo: 'Muzhi1920.github.io',
      owner: 'Muzhi1920',
      admin: 'Muzhi1920',
      id: 'Mon Mar 21 2022 10:34:17 GMT+0000 | truncate: 50', // Ensure uniqueness and length less than 50
      distractionFreeMode: false, // Facebook-like distraction free mode
      perPage: 10,
      pagerDirection: 'last',
      createIssueManually: false,
      language: 'zh-CN',
      proxy: 'https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token'
    });
    gitalk.render('gitalk-container');

    var gtFolded = () => {
      setTimeout(function () {
        let markdownBody = document.getElementsByClassName("markdown-body");
        let list = Array.from(markdownBody);
        list.forEach(item => {
          if (item.clientHeight > 250) {
            item.classList.add('gt-comment-body-folded');
            item.style.maxHeight = '250px';
            item.title = 'Click to Expand';
            item.onclick = function () {
              item.classList.remove('gt-comment-body-folded');
              item.style.maxHeight = '';
              item.title = '';
              item.onclick = null;
            };
          }
        })
      }, 800);
    }
  </script>

  <!-- gitalk end -->


<!-- 2. gitment comment -->


<!-- 3. disqus comment -->


        <!-- comments end -->
        <hr>

      </div>

      <!-- Catalog: Tabe of Content -->
      <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">目录</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E7%AC%AC6%E7%AB%A0-%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">第6章 逻辑斯谛回归</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#scikit-learn%E5%AE%9E%E4%BE%8B"><span class="toc-nav-number">1.0.1.</span> <span class="toc-nav-text">scikit-learn实例</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#sklearn-linear-model-LogisticRegression"><span class="toc-nav-number">1.0.1.1.</span> <span class="toc-nav-text">sklearn.linear_model.LogisticRegression</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E7%AC%AC6%E7%AB%A0Logistic%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B-%E4%B9%A0%E9%A2%98"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text">第6章Logistic回归与最大熵模型-习题</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%B9%A0%E9%A2%986-1"><span class="toc-nav-number">1.1.1.</span> <span class="toc-nav-text">习题6.1</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%B9%A0%E9%A2%986-2"><span class="toc-nav-number">1.1.2.</span> <span class="toc-nav-text">习题6.2</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%B9%A0%E9%A2%986-3"><span class="toc-nav-number">1.1.3.</span> <span class="toc-nav-text">习题6.3</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">优化算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">梯度下降</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%89%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Batch-SGD%EF%BC%89-VS-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-nav-number">2.1.1.</span> <span class="toc-nav-text">批梯度下降（Batch SGD） VS  随机梯度下降</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88mini-batch-SGD%EF%BC%89"><span class="toc-nav-number">2.1.2.</span> <span class="toc-nav-text">小批量随机梯度下降（mini batch SGD）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E2%80%9C%E6%89%B9%E2%80%9D%E7%9A%84%E5%A4%A7%E5%B0%8F%E5%AF%B9%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-nav-number">2.1.3.</span> <span class="toc-nav-text">“批”的大小对优化效果的影响</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-nav-number">2.1.4.</span> <span class="toc-nav-text">随机梯度下降存在的问题</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%94%B9%E8%BF%9B%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">改进的随机梯度下降</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%8A%A8%E9%87%8F%E7%AE%97%E6%B3%95"><span class="toc-nav-number">2.2.1.</span> <span class="toc-nav-text">动量算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Momentum%E5%8A%A8%E9%87%8F"><span class="toc-nav-number">2.2.1.1.</span> <span class="toc-nav-text">Momentum动量</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#NAG-%E5%8A%A8%E9%87%8F"><span class="toc-nav-number">2.2.1.2.</span> <span class="toc-nav-text">NAG 动量</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Momentum%E5%92%8CNAG%E5%8A%A8%E9%87%8F%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-nav-number">2.2.1.3.</span> <span class="toc-nav-text">Momentum和NAG动量的区别</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-nav-number">2.2.2.</span> <span class="toc-nav-text">自适应学习率的优化算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#AdaGrad"><span class="toc-nav-number">2.2.2.1.</span> <span class="toc-nav-text">AdaGrad</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#AdaGrad-%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="toc-nav-number">2.2.2.1.1.</span> <span class="toc-nav-text">AdaGrad 算法描述</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#AdaGrad-%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-nav-number">2.2.2.1.2.</span> <span class="toc-nav-text">AdaGrad 存在的问题</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#RMSProp"><span class="toc-nav-number">2.2.2.2.</span> <span class="toc-nav-text">RMSProp</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#RMSProp-%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="toc-nav-number">2.2.2.2.1.</span> <span class="toc-nav-text">RMSProp 算法描述</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Adam"><span class="toc-nav-number">2.2.2.3.</span> <span class="toc-nav-text">Adam</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#Adam-%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="toc-nav-number">2.2.2.3.1.</span> <span class="toc-nav-text">Adam 算法描述</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E8%BF%99%E4%BA%9B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">如何选择这些优化算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%90%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-nav-number">2.3.1.</span> <span class="toc-nav-text">各优化算法的可视化</span></a></li></ol></li></ol></li></ol>
        
        </div>
      </aside>
    



      <!-- Sidebar Container -->
      <div class="
                col-lg-8 col-lg-offset-1
                col-md-10 col-md-offset-1
                sidebar-container">

        <!-- Featured Tags -->
        
        <section>
          <!-- no hr -->
          <h5>
            <a href="/tags/">特色标签</a>
          </h5>
          <div class="tags">
            
            <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
            
          </div>
        </section>
        

        <!-- Friends Blog -->
        
        <hr>
        <h5>链友</h5>
        <ul class="list-inline">

          
          <li>
            <a href="https://hexo.io/" target="_blank">Hexo</a>
          </li>
          
        </ul>
        
      </div>
    </div>
  </div>
</article>



<!-- anchorjs start -->
<!-- async load function -->
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script type="text/javascript">
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function(e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  };
</script>
<script type="text/javascript">
  //anchor-js, Doc:http://bryanbraun.github.io/anchorjs/
  async ("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js", function() {
    anchors.options = {
      visible: 'hover',
      placement: 'left',
      // icon: 'ℬ'
      icon: '❡'
    };
    anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
  });
</script>
<style>
  /* place left on bigger screen */
  @media all and (min-width: 800px) {
    .anchorjs-link {
      position: absolute;
      left: -0.75em;
      font-size: 1.1em;
      margin-top: -0.1em;
    }
  }
</style>

<!-- anchorjs end -->



		<!-- Footer (contains ThemeColor、viewer) -->
		<!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center">
          

          
            <li>
              <a target="_blank" href="https://github.com/Muzhi1920">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          

          

          

          

          

          
            <li>
              <a target="_blank" href="https://www.zhihu.com/people/awesome-yyds">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa  fa-stack-1x fa-inverse">知</i>
                </span>
              </a>
            </li>
          

          

        </ul>
        <p class="copyright text-muted">
          Copyright &copy;
          小于
          2022
          <br>
          Theme by
          <a target="_blank" rel="noopener" href="https://hexo.io/themes/">Hexo</a>
          <span style="display: inline-block; margin: 0 5px;">
            <i class="fa fa-heart"></i>
          </span>
          re-Ported by
          <a target="_blank" rel="noopener" href="https://github.com/Muzhi1920/Muzhi1920.github.io">Muzhi1920</a>
          |
          <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=Muzhi1920&repo=Muzhi1920.github.io&type=star&count=true"></iframe>
        </p>
      </div>
    </div>
  </div>
</footer>

<a id="rocket" href="#top" class=""></a>


  <!-- jQuery -->
  <script type="text/javascript" src="/js/jquery.min.js"></script>
  <!-- Bootstrap Core JavaScript -->
  <script type="text/javascript" src="/js/bootstrap.min.js"></script>
  <!-- Custom Theme JavaScript -->
  <script type="text/javascript" src="/js/hux-blog.min.js"></script>
  <!-- catalog -->
  <script async="true" type="text/javascript" src="/js/catalog.js"></script>
  <!-- totop(rocket) -->
  <script async="true" type="text/javascript" src="/js/totop.js"></script>

  
    <!-- Busuanzi JavaScript -->
    <script async="async" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

  
    <!-- Scroll start -->
    <script async="async" type="text/javascript" src="/js/scroll.js"></script>
    <!-- Scroll end -->
  

  
    <!-- LangSelect start -->
    <script type="text/javascript" src="/js/langselect.js"></script>
    <!-- LangSelect end -->
  

  
    <!-- Mouseclick -->
    <script type="text/javascript" src="/js/mouseclick.js" content='统计学习方法三要素——模型、策略、算法,高斯证明了假定误差独立同分布，在所有无偏线性估计中，最小二乘法的方差最小,SGD算法：通过误分类点优化w，b的值，使超平面向另一侧移动,朴素贝叶斯法的基本假设是条件独立性（强假设所以叫“朴素”）,决策树特征选择：信息增益、信息增益比，基尼指数（越小越好）,逻辑回归是用线性变换表示输入的对数几率模型,最大熵原理认为在所有概率模型中，熵最大的模型是最好的模型,逻辑回归与最大熵模型都属于对数线性模型，采用极大似然估计学习,逻辑回归的优化，包括改进的迭代尺度法、梯度下降法、拟牛顿法' color='#9933CC,#339933,#66CCCC,#FF99CC,#CCCCFF,#6666CC,#663399,#66CC99,#FF0033'></script>
  

  
    <!-- ribbon -->
    <script type="text/javascript" src="/js/ribbonDynamic.js"></script>
  

  






  <!-- viewer start -->
  <!-- viewer start (Picture preview) -->
  
    <script async="async" type="text/javascript" src="/js/viewer/viewer.min.js"></script>
    <script async="async" type="text/javascript" src="/js/viewer/pic-viewer.js"></script>
  

  <!-- viewer end -->


<script>
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function (e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  }

  // fastClick.js
  async ("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function () {
    var $nav = document.querySelector("nav");
    if ($nav)
      FastClick.attach($nav);
    }
  )
</script>

<!-- Because of the native support for backtick-style fenced code blocks right within the Markdown is landed in Github Pages, From V1.6, There is no need for Highlight.js, so Huxblog drops it officially. -
https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0 - https://help.github.com/articles/creating-and-highlighting-code-blocks/ -->
<!-- <script> async ("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function () { hljs.initHighlightingOnLoad(); }) </script> <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet"> -->

<!-- jquery.tagcloud.js -->
<!-- <script> // only load tagcloud.js in tag.html if ($('#tag_cloud').length !== 0) { async ("https://11010101.xyz/js/jquery.tagcloud.js", function () { $.fn.tagcloud.defaults = { // size: { start: 1, end: 1, unit: 'em' }, color: {
start: '#bbbbee', end: '#0085a1' } }; $('#tag_cloud a').tagcloud(); }) } </script> -->


		<!-- Search -->
		
		<div class="popup search-popup local-search-popup">
  <span class="popup-btn-close">
    ESC
  </span>
  <div class="container">
    <div class="row">
      <!-- <div class="col-md-9 col-md-offset-1"> -->
      <div class="col-lg-9 col-lg-offset-1 col-md-10 col-md-offset-1 local-search-content">

        <div class="local-search-header clearfix">

          <div class="local-search-input-wrapper">
            <span class="search-icon">
              <i class="fa fa-search fa-lg" style="margin: 25px 10px 25px 20px;"></i>
            </span>
            <input autocomplete="off" placeholder="搜索..." type="text" id="local-search-input">
          </div>
        </div>
        <div id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>


  
    <script src="/js/ziploader.js"></script>
  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    // monitor main search box;
    var onPopupClose = function (e) {
      $('.popup').fadeOut(300);
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $('.popup').fadeIn(300);
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }
    // get search zip version
    $.get('/searchVersion.json?t=' + (+new Date()), function (res) {
      if (localStorage.getItem('searchVersion') !== res) {
        localStorage.setItem('searchVersion', res);
        initSearchJson();
      }
    });

    function initSearchJson() {
      initLoad(['/search.flv'], {
        loadOptions: {
          success: function (obj) {
            localStorage.setItem('searchJson', obj['search.json'])
          },
          error: function (e) {
            return console.log(e)
          }
        },
        returnOptions: {
          'json': TYPE_TEXT
        },
        mimeOptions: {
          'json': 'application/json'
        }
      })
    }
    // search function;
    var searchFunc = function (search_id, content_id) {
      'use strict';
      isfetched = true;
      var datas = JSON.parse(localStorage.getItem('searchJson'));
      // console.log(search_id)
      var input = document.getElementById(search_id);
      var resultContent = document.getElementById(content_id);
      var inputEventFunction = function () {
        var searchText = input.value.trim().toLowerCase();
        var keywords = searchText.split(/[\s\-]+/);
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        var resultItems = [];
        if (searchText.length > 0) {
          // perform local searching
          datas.forEach(function (data) {
            var isMatch = false;
            var hitCount = 0;
            var searchTextCount = 0;
            var title = data.title
              ? data.title.trim()
              : '';
            var titleInLowerCase = title.toLowerCase();
            var content = data.content
              ? data.content.trim().replace(/<[^>]+>/g, "")
              : '';
            var contentInLowerCase = content.toLowerCase();
            var articleUrl = decodeURIComponent(data.url);

            var date = data.date;
            var dateTime = date.replace(/T/, " ").replace(/.000Z/, "");
            var imgUrl = data.header_img;
            


            var indexOfTitle = [];
            var indexOfContent = [];
            // only match articles with not empty titles
            keywords.forEach(function (keyword) {
              function getIndexByWord(word, text, caseSensitive) {
                var wordLen = word.length;
                if (wordLen === 0) {
                  return [];
                }
                var startPosition = 0,
                  position = [],
                  index = [];
                if (!caseSensitive) {
                  text = text.toLowerCase();
                  word = word.toLowerCase();
                }
                while ((position = text.indexOf(word, startPosition)) > -1) {
                  index.push({position: position, word: word});
                  startPosition = position + wordLen;
                }
                return index;
              }
              indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
              indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
            });
            if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
              isMatch = true;
              hitCount = indexOfTitle.length + indexOfContent.length;
            }
            // show search results
            if (isMatch) {
              // sort index by position of keyword
              [indexOfTitle, indexOfContent].forEach(function (index) {
                index.sort(function (itemLeft, itemRight) {
                  if (itemRight.position !== itemLeft.position) {
                    return itemRight.position - itemLeft.position;
                  } else {
                    return itemLeft.word.length - itemRight.word.length;
                  }
                });
              });
              // merge hits into slices
              function mergeIntoSlice(text, start, end, index) {
                var item = index[index.length - 1];
                var position = item.position;
                var word = item.word;
                var hits = [];
                var searchTextCountInSlice = 0;
                while (position + word.length <= end && index.length != 0) {
                  if (word === searchText) {
                    searchTextCountInSlice++;
                  }
                  hits.push({position: position, length: word.length});
                  var wordEnd = position + word.length;
                  // move to next position of hit
                  index.pop();
                  while (index.length != 0) {
                    item = index[index.length - 1];
                    position = item.position;
                    word = item.word;
                    if (wordEnd > position) {
                      index.pop();
                    } else {
                      break;
                    }
                  }
                }
                searchTextCount += searchTextCountInSlice;
                return {hits: hits, start: start, end: end, searchTextCount: searchTextCountInSlice};
              }
              var slicesOfTitle = [];
              if (indexOfTitle.length != 0) {
                slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
              }
              var slicesOfContent = [];
              while (indexOfContent.length != 0) {
                var item = indexOfContent[indexOfContent.length - 1];
                var position = item.position;
                var word = item.word;
                // cut out 100 characters
                var start = position - 20;
                var end = position + 80;
                if (start < 0) {
                  start = 0;
                }
                if (end < position + word.length) {
                  end = position + word.length;
                }
                if (end > content.length) {
                  end = content.length;
                }
                slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
              }
              // sort slices in content by search text's count and hits' count
              slicesOfContent.sort(function (sliceLeft, sliceRight) {
                if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                  return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                  return sliceRight.hits.length - sliceLeft.hits.length;
                } else {
                  return sliceLeft.start - sliceRight.start;
                }
              });
              // select top N slices in content
              var upperBound = parseInt('1');
              if (upperBound >= 0) {
                slicesOfContent = slicesOfContent.slice(0, upperBound);
              }
              // highlight title and content
              function highlightKeyword(text, slice) {
                var result = '';
                var prevEnd = slice.start;
                slice.hits.forEach(function (hit) {
                  result += text.substring(prevEnd, hit.position);
                  var end = hit.position + hit.length;
                  result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                  prevEnd = end;
                });
                result += text.substring(prevEnd, slice.end);
                return result;
              }
              var resultItem = '';

              // if (slicesOfTitle.length != 0) {   resultItem += "<li><a target='_blank' href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>"; } else {   resultItem += "<li><a target='_blank' href='" +
              // articleUrl + "' class='search-result-title'>" + title + "</a>"; } slicesOfContent.forEach(function (slice) {   resultItem += "<a target='_blank' href='" + articleUrl + "'><p class=\"search-result\">" + highlightKeyword(content, slice) +
              // "...</p></a>"; }); resultItem += "</li>";

              if (slicesOfTitle.length != 0) {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</div><time class='search-result-date'>" + dateTime + "</time>";
              } else {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + title + "</div><time class='search-result-date'>" + dateTime + "</time>";
              }
              slicesOfContent.forEach(function (slice) {
                resultItem += "<p class=\"search-result-content\">" + highlightKeyword(content, slice) + "...</p>";
              });
              resultItem += "</div><div class='search-result-right'><img class='media-image' src='" + imgUrl + "' width='64px' height='48px'></img></div></a>";

              resultItems.push({item: resultItem, searchTextCount: searchTextCount, hitCount: hitCount, id: resultItems.length});
            }
          })
        };

        if (keywords.length === 1 && keywords[0] === "") {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else if (resultItems.length === 0) {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else {
          resultItems.sort(function (resultLeft, resultRight) {
            if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
              return resultRight.searchTextCount - resultLeft.searchTextCount;
            } else if (resultLeft.hitCount !== resultRight.hitCount) {
              return resultRight.hitCount - resultLeft.hitCount;
            } else {
              return resultRight.id - resultLeft.id;
            }
          });
          var searchResultList = '<div class=\"search-result-list\">';
          resultItems.forEach(function (result) {
            searchResultList += result.item;
          })
          searchResultList += "</div>";
          resultContent.innerHTML = searchResultList;
        }
      }
      if ('auto' === 'auto') {
        input.addEventListener('input', inputEventFunction);
      } else {
        $('.search-icon').click(inputEventFunction);
        input.addEventListener('keypress', function (event) {
          if (event.keyCode === 13) {
            inputEventFunction();
          }
        });
      }
      // remove loading animation
      $('body').css('overflow', '');
      proceedsearch();
    }
    // handle and trigger popup window;
    $('.popup-trigger').click(function (e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc('local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });
    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function (e) {
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 && $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });

    document.addEventListener('mouseup', (e) => {
      var _con = document.querySelector(".local-search-content");
      if (_con) {
        if (!_con.contains(e.target)) {
          onPopupClose();
        }
      }
    });
  </script>


		
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
